# coding=UTF-8
# Rank List æ ‡æ³¨å¹³å°ï¼Œç”¨äºæ ‡æ³¨ Reward Model çš„è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡streamlitæ­å»ºã€‚

'''
@File: answer_rank_labeler
@Author: WeiWei
@Time: 2023/3/5
@Email: weiwei_519@outlook.com
@Software: PyCharm
'''

import os
import streamlit as st
import torch
import json
from PIL import Image
import logging
import datetime
from transformers import AutoTokenizer, GPT2LMHeadModel, TextGenerationPipeline
from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config
from model.T5_model import infer_answer
from steamship import Steamship
# from streamlit_autorefresh import st_autorefresh
import requests as req
import urllib3

urllib3.disable_warnings()

today = datetime.datetime.now().strftime("%Y-%m-%d")
logging.basicConfig(filename=f'./logs/labeler/labeler_{today}.log',
                    level=logging.INFO,
                    format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    filemode='a'
                    )

st.set_page_config(
    page_title="AIA-GPT ç®¡ç†å¹³å°",
    page_icon="ğŸ¦ˆ",
    layout="wide"
)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

MODEL_CONFIG = {
    'model_name': './models/chatgpt-aia-chinese/ttt-aia-chinese',  # backbone
    'dataset_file': './datasets/company_datasets/human_rank_pairs.json',  # æ ‡æ³¨æ•°æ®é›†çš„å­˜æ”¾æ–‡ä»¶
    'rank_list_len': 3,  # æ’åºåˆ—è¡¨çš„é•¿åº¦
    'max_gen_seq_len': 400,  # ç”Ÿæˆç­”æ¡ˆæœ€å¤§é•¿åº¦
}

######################## é¡µé¢é…ç½®åˆå§‹åŒ– ###########################
RANK_COLOR = [
    'red',
    'green',
    'blue',
    'orange',
    'violet'
]

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š
# tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['model_name'])
# model = GPT2LMHeadModel.from_pretrained(MODEL_CONFIG['model_name'])
# TextGenerationPipeline(model, tokenizer, device=device)
# model.to(device)

# åŠ è½½Companyé¢„è®­ç»ƒæ¨¡å‹
# if 'model' not in st.session_state:
#     tokenizer = T5Tokenizer.from_pretrained(MODEL_CONFIG['model_name'])
#     config = T5Config.from_pretrained(MODEL_CONFIG['model_name'])
#     model = T5ForConditionalGeneration.from_pretrained(MODEL_CONFIG['model_name'])
#     model.to(device)
#     st.session_state['tokenizer'] = tokenizer
#     st.session_state['config'] = config
#     st.session_state['model'] = model

######################## ä¼šè¯ç¼“å­˜åˆå§‹åŒ– ###########################

if 'aiagpt_prompt' not in st.session_state:
    st.session_state['aiagpt_prompt'] = ''

if 'aiagpt_query' not in st.session_state:
    st.session_state['aiagpt_query'] = ''

if 'aiagpt_answer' not in st.session_state:
    st.session_state['aiagpt_answer'] = ''

if 'chatgpt_prompt' not in st.session_state:
    st.session_state['chatgpt_prompt'] = ''

if 'chatgpt_query' not in st.session_state:
    st.session_state['chatgpt_query'] = ''

if 'chatgpt_answer' not in st.session_state:
    st.session_state['chatgpt_answer'] = ''

if 'answer_to_rank' not in st.session_state:
    answer_to_rank = {}
    st.session_state['answer_to_rank'] = answer_to_rank

if 'rank_choices' not in st.session_state:
    rank_choices = ['-1']
    for i in range(MODEL_CONFIG['rank_list_len']):
        rank_choices.append(str(i + 1))
    st.session_state['rank_choices'] = rank_choices

# åŠ è½½GPT4 client
if 'GPT4_client' not in st.session_state:
    # client = Steamship(workspace="gpt-4", api_key='E7A97033-8208-452F-945D-782EA5E011AC', verify=False)
    # # create text generator from steamship client
    # generator = client.use_plugin('gpt-4')
    # st.session_state['GPT4_client'] = generator
    st.session_state['GPT4_client'] = ''

if 'chatgpt_program_prompt' not in st.session_state:
    st.session_state['chatgpt_program_prompt'] = ''

if 'chatgpt_program_answer' not in st.session_state:
    st.session_state['chatgpt_program_answer'] = ''


######################### å‡½æ•°å®šä¹‰åŒº ##############################
def generate_text(prompt, do_sample):
    """
    æ¨¡å‹ç”Ÿæˆæ–‡å­—ã€‚
    """
    logging.info(f'AIA-GPT prompt: {prompt}')
    logging.info('åŠ è½½AIAGPT model')
    tokenizer = T5Tokenizer.from_pretrained(MODEL_CONFIG['model_name'])
    model = T5ForConditionalGeneration.from_pretrained(MODEL_CONFIG['model_name'])
    model.to(device)
    logging.info('AIAGPT modelåŠ è½½æˆåŠŸ')
    # current_results = []
    # for _ in range(MODEL_CONFIG['rank_list_len']):
    #     if st.session_state['current_prompt'] != '':
    #         res = st.session_state['generator'](
    #             st.session_state['current_prompt'],
    #             max_length=MODEL_CONFIG['max_gen_seq_len'],
    #             do_sample=True,
    #             early_stopping=True,  # ä¿è¯å½“é‡åˆ°EOSè¯æ—¶ï¼Œç»“æŸç”Ÿæˆ
    #         )
    #         current_results.extend([e['generated_text'] for e in res])
    # st.session_state['current_results'] = current_results
    return_seqs = MODEL_CONFIG['rank_list_len']
    if do_sample:
        # æ ‡æ³¨æ¨¡å¼
        action = 'æ ‡æ³¨æ¨¡å¼'
        return_seqs = MODEL_CONFIG['rank_list_len']
        logging.info(f'action: {action}')
    else:
        # é—®ç­”æ¨¡å¼
        action = 'é—®ç­”æ¨¡å¼'
        return_seqs = 1
        logging.info(f'action: {action}')
    answers = infer_answer(text=prompt,
                           tokenizer=tokenizer,
                           model=model,
                           # do_sample=do_sample,
                           samples=return_seqs,
                           out_length=MODEL_CONFIG['max_gen_seq_len'],
                           )
    answers = [answer.replace('\n', '<br>').replace('\t', '   ') for answer in answers]
    logging.info(f'answer in generator: {len(answers)} x {len(answers[0])}')
    logging.info(f'answer in generator: {answers}')
    if action == 'æ ‡æ³¨æ¨¡å¼':
        st.session_state['answer_to_rank'] = {}
        for _, answer in enumerate(answers):
            st.session_state['answer_to_rank'][answer] = '-1'  # -1 ä¸ºåˆå§‹rankï¼Œç­‰äºæœªæ’åº
    elif action == 'é—®ç­”æ¨¡å¼':
        st.session_state['aiagpt_answer'] = answers[0]
    del model, tokenizer
    torch.cuda.empty_cache()


def generate_from_GPT4(prompt):
    logging.info(f'GPT4 prompt: {prompt}')
    # generator = st.session_state['GPT4_client']
    # task = generator.generate(text=prompt)
    # task.wait()
    # response = task.output.blocks[0].text
    # print(f'GPT4 answer is: {response}')
    st.session_state['gpt4_program_answer'] = ''


def generate_from_GPT3_from_JJY(prompt):
    URL = 'https://openapi.jijyun.cn/api/openapi/corp_token'
    req_body = {
        "corp_id": "gLnZxt6M21cPBoV6wIT6xYBNTIhv56Av",
        "secret": "fE6btX6T6GluuN6b5B7MWckV2wZtYv5w",
        "token": prompt
    }

    response = req.post(url=URL, data=req_body)
    st.session_state['chatgpt_answer'] = response


######################### é¡µé¢å®šä¹‰åŒºï¼ˆä¾§è¾¹æ ï¼‰ ########################
with st.sidebar:
    st.image(Image.open('./images/AIA_logo.png'))
    st.sidebar.title('ğŸ“Œ AIA-GPT ç®¡ç†å¹³å°')
    st.sidebar.markdown('''
    ```python
    æä¾›åŸºäºGPTæ¨¡å‹çš„ç”Ÿæˆå¼å†…å®¹æé—®:
    
    Tab 1:
        Prompt Tabï¼šé—®ç­”æ¨¡å¼ã€‚
            ç”¨æˆ·è¾“å…¥æç¤ºé—®é¢˜ï¼Œç³»ç»Ÿè¾“å‡ºå”¯ä¸€ç­”æ¡ˆã€‚
    
    Tab 2:
        Label Tabï¼šæ ‡æ³¨æ¨¡å¼ã€‚
            ç”¨æˆ·è¾“å…¥æç¤ºé—®é¢˜ï¼Œç³»ç»Ÿè¾“å‡ºå¤šä¸ªç­”æ¡ˆã€‚
            ç”¨æˆ·æ ¹æ®ç­”æ¡ˆçš„å‡†ç¡®æ€§è¿›è¡Œæ’åºã€‚
            
    Tab 3:
        Programï¼šä»£ç è¾…åŠ©å·¥å…·
            åŸºäºéœ€æ±‚ç”Ÿæˆä»£ç ã€‚
            åŸºäºä»£ç ï¼Œæä¾›è§£è¯»ã€‚
            ä»£ç bugå‘ç°ä¸ä¿®å¤å»ºè®®ã€‚
            ä»£ç å•å…ƒæµ‹è¯•å»ºè®®ã€‚
            ä¸ºæ—¢æœ‰ä»£ç æ·»åŠ æ³¨é‡Šã€‚
    
    ```
    ''')

AIAGPT_prompt_tab, AIAGPT_label_tab, ChatGPT_prompt_tab, ChatGPT_program_tab = st.tabs(
    ['AIA-GPT-Prompt', 'AIA-GPT-Label', 'ChatGPT-Prompt', 'ChatGPT-Program'])
answer_to_rank = []
st.write('<style>div.row-widget.stRadio > div{flex-direction:row;justify-content: left;} </style>',
         unsafe_allow_html=True)

# st.write('<style>div.st-bf{flex-direction:row;} div.st-ag{font-weight:bold;padding-left:2px;}</style>',
#          unsafe_allow_html=True)

######################### é¡µé¢å®šä¹‰åŒºï¼ˆé—®ç­”é¡µé¢ï¼‰ ########################
with AIAGPT_prompt_tab:
    st.subheader(':blue[AIA-GPTå¯¹è¯é—®ç­”]')
    with st.expander('', expanded=True):  # expanded æ˜¯å±•å¼€è¿˜æ˜¯æ”¶ç¼©çŠ¶æ€
        qa_query_txt = st.text_area('ğŸ” è¯·è¾“å…¥æ‚¨çš„æé—®ï¼š', placeholder='æ­¤å¤„è¾“å…¥æ‚¨çš„æé—®', key='qa_query')
        if qa_query_txt != '' and st.session_state['aiagpt_prompt'] != qa_query_txt:
            st.session_state['aiagpt_prompt'] = qa_query_txt
            generate_text(st.session_state['aiagpt_prompt'], do_sample=False)

    with st.expander('ğŸ’¡ GPTçš„å›ç­”å¦‚ä¸‹ï¼š', expanded=True):
        if 'aiagpt_answer' in st.session_state:
            answer = st.session_state['aiagpt_answer']
            if answer != '':
                color = RANK_COLOR[0]
                st.markdown(f":{color}[{answer}]", unsafe_allow_html=True)

######################### é¡µé¢å®šä¹‰åŒºï¼ˆæ ‡æ³¨é¡µé¢ï¼‰ ########################
with AIAGPT_label_tab:
    st.subheader(':blue[AIA-GPTå¯¹è¯é—®ç­”]')
    with st.expander('', expanded=True):  # expanded æ˜¯å±•å¼€è¿˜æ˜¯æ”¶ç¼©çŠ¶æ€
        label_query_txt = st.text_area('ğŸ” è¯·è¾“å…¥æ‚¨çš„æé—®ï¼š', placeholder='æ­¤å¤„è¾“å…¥æ‚¨çš„æé—®', key='label_query')
        if label_query_txt != '' and st.session_state['aiagpt_query'] != label_query_txt:
            st.session_state['aiagpt_query'] = label_query_txt
            generate_text(st.session_state['aiagpt_query'], do_sample=True)

    with st.expander('ğŸ’¡ GPTçš„å›ç­”å¦‚ä¸‹ï¼š', expanded=True):
        answer_to_rank = st.session_state['answer_to_rank']
        if len(answer_to_rank) > 0:
            rank_choices = st.session_state['rank_choices']
            i = 0
            for answer, rank in answer_to_rank.items():
                if rank != '0':
                    rank_col, answer_col = st.columns([1, 10])
                    i += 1
                    curr_choice = int(rank) if rank != '-1' else 0
                    with rank_col:
                        st.text("ğŸ¥‡ Choose Rank")
                        choice = st.selectbox(f'å¥å­{i}æ’å', rank_choices,
                                              help='ä¸ºå½“å‰å¥å­é€‰æ‹©æ’åï¼Œæ’åè¶Šå°ï¼Œå¾—åˆ†è¶Šé«˜ã€‚ï¼ˆ-1ä»£è¡¨å½“å‰å¥å­æš‚æœªè®¾ç½®æ’åï¼‰')
                        if choice != curr_choice:
                            st.session_state['answer_to_rank'][answer] = choice
                            curr_choice = choice
                    with answer_col:
                        st.text("ğŸ’¡ Generated Answers")
                        if curr_choice != '-1':
                            color = RANK_COLOR[int(curr_choice) - 1]
                            st.markdown(f":{color}[{answer}]")
                        else:
                            color = 'white'
                            st.markdown(f":{color}[{answer}]", unsafe_allow_html=True)
                # st.session_state['answer_to_rank'] = answer_to_rank

            # æ‰‹å·¥å†™ç­”æ¡ˆ
            rank_col, answer_col = st.columns([1, 10])
            rank = MODEL_CONFIG['rank_list_len'] + 1
            with rank_col:
                st.text("ğŸ¥‡ Choose Rank")
                st.text('0')
            with answer_col:
                input_answer = st.text_area(label='ğŸ’¡ è¯·è¾“å…¥æ‚¨çš„ç­”æ¡ˆï¼š', placeholder='è¿™é‡Œæ‰‹å·¥è¾“å…¥ç­”æ¡ˆ')
                add_button = st.button('ï¼‹æ·»åŠ æ‰‹å·¥ç­”æ¡ˆ')
                if input_answer != '' and add_button:
                    st.session_state['answer_to_rank'][input_answer] = '0'
                    st.success('æˆåŠŸæ·»åŠ æ‰‹å·¥ç­”æ¡ˆï¼Œè¯·ç‚¹å‡»æŒ‰é’®å­˜å‚¨å½“å‰æ’åº', icon="âœ…")

        else:
            rank_col, answer_col = st.columns([1, 10])
            with rank_col:
                st.text("ğŸ¥‡ Choose Rank")
            with answer_col:
                st.text("ğŸ’¡ Generate Answers")

    answer_to_rank_new = st.session_state['answer_to_rank']

    save_button = st.button('å­˜å‚¨å½“å‰æ’åº')
    if save_button:
        answer_to_rank = st.session_state['answer_to_rank']
        rank_list = []
        for ans, rank in answer_to_rank.items():
            if rank == '-1':
                st.error('è¯·å®Œæˆæ’åºåå†å­˜å‚¨ï¼', icon='ğŸš¨')  # æ‰€æœ‰æ ‡æ³¨éƒ½å·²æ’åºï¼Œæ‰èƒ½ä¿å­˜
                st.stop()
            if rank in rank_list:
                st.error('åºå·é€‰æ‹©é‡å¤ï¼', icon='ğŸš¨')  # åºå·é€‰æ‹©é‡å¤
                st.stop()
            else:
                rank_list.append(rank)

        file = open(MODEL_CONFIG['dataset_file'], 'r+', encoding='utf8')
        content = file.read()
        # logging.info(f'orignal file content: {content}')
        file.seek(0)
        file.truncate()
        if len(content) != 0 and content != '':
            rank_pairs = json.loads(content)
        else:
            rank_pairs = {}
        length = len(rank_pairs)
        logging.info(f'orignal json length: {length}')
        rank_pairs[length] = {}
        rank_pairs[length]['prompt'] = st.session_state['prompt'].strip()
        ranking = {}
        for ans, rank in answer_to_rank.items():
            ranking[rank] = ans
        ranked_answers = [ranking[key] for key in sorted(ranking.keys())]
        rank_pairs[length]['ranked_answers'] = ranked_answers
        # dumps()ï¼šå°†dictæ•°æ®è½¬åŒ–æˆjsonæ•°æ®ï¼›   dump()ï¼šå°†dictæ•°æ®è½¬åŒ–æˆjsonæ•°æ®åå†™å…¥jsonæ–‡ä»¶
        content = json.dumps(rank_pairs, ensure_ascii=False, indent=2)
        logging.info(f'file save content length {len(content)}')
        # json.dump(content, file, ensure_ascii=False, indent=2)
        file.write(content)
        file.flush()
        st.success('ä¿å­˜æˆåŠŸï¼Œè¯·æ›´æ¢promptç”Ÿæˆæ–°çš„ç­”æ¡ˆ~', icon="âœ…")
        st.session_state.clear()
        # label_tab.empty()
        # st._rerun()
        # st_autorefresh(interval=2000, limit=100, key="fizzbuzzcounter")

######################### é¡µé¢å®šä¹‰åŒºï¼ˆChatGPT APIé¡µé¢ï¼‰ ########################
with ChatGPT_prompt_tab:
    st.subheader(':blue[ChatGPTå¯¹è¯é—®ç­”]')
    with st.expander('', expanded=True):  # expanded æ˜¯å±•å¼€è¿˜æ˜¯æ”¶ç¼©çŠ¶æ€
        cg_qa_query_txt = st.text_area('ğŸ” è¯·è¾“å…¥æ‚¨çš„æé—®ï¼š', placeholder='æ­¤å¤„è¾“å…¥æ‚¨çš„æé—®', key='qa_query')
        if cg_qa_query_txt != '' and st.session_state['chatgpt_prompt'] != cg_qa_query_txt:
            st.session_state['chatgpt_prompt'] = cg_qa_query_txt
            generate_from_GPT3_from_JJY(st.session_state['chatgpt_prompt'])

    with st.expander('ğŸ’¡ GPTçš„å›ç­”å¦‚ä¸‹ï¼š', expanded=True):
        if 'chatgpt_answer' in st.session_state:
            answer = st.session_state['chatgpt_answer']
            if answer != '':
                color = RANK_COLOR[0]
                st.markdown(f":{color}[{answer}]", unsafe_allow_html=True)

######################### é¡µé¢å®šä¹‰åŒºï¼ˆä»£ç è¾…åŠ©å·¥å…·é¡µé¢ï¼‰ ########################
with ChatGPT_program_tab:
    st.subheader(':blue[æä¾›é¢å‘ITå¼€å‘äººå‘˜çš„ä»£ç è¾…åŠ©å·¥å…·]')
    with st.expander('', expanded=True):  # expanded æ˜¯å±•å¼€è¿˜æ˜¯æ”¶ç¼©çŠ¶æ€
        func_options = ('è¯·é€‰æ‹©ï¼š', 'ç”Ÿæˆä»£ç ', 'ä»£ç è§£è¯»', 'ä¿®å¤bug', 'å•å…ƒæµ‹è¯•', 'æ·»åŠ æ³¨é‡Š')
        # gene_tab, read_tab, bug_tab, test_tab, comment_tab = st.tabs(func_options)
        func_radio = st.radio(label='è¯·é€‰æ‹©æ‚¨çš„éœ€æ±‚ï¼š',
                              options=func_options,
                              index=0,
                              key='func_options',
                              )
        lang_options = ('Java', 'SQL', 'Vue.js', 'Python', 'C++', 'Cobol', 'Swift')
        lang_radio = st.radio(label='è¯·é€‰æ‹©æ‚¨ä»£ç çš„è¯­è¨€ï¼š',
                              options=lang_options,
                              index=0,
                              key='lang_options_gene',
                              )
        prompt = ""
        if func_radio == 'ç”Ÿæˆä»£ç ':
            prompt = "è¯·å¸®æˆ‘ç”Ÿæˆä¸€æ®µ" + lang_radio + "ä»£ç ï¼Œéœ€æ±‚æ˜¯ï¼š\n"
        elif func_radio == 'ä»£ç è§£è¯»':
            prompt = "è¯·å¸®æˆ‘æ€»ç»“å¦‚ä¸‹" + lang_radio + "ä»£ç çš„ç”¨é€”ï¼Œä»£ç æ˜¯ï¼š\n"
        elif func_radio == 'ä¿®å¤bug':
            prompt = "è¯·å¸®æˆ‘å‘ç°å¹¶ä¿®å¤å¦‚ä¸‹" + lang_radio + "ä»£ç ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œä»£ç æ˜¯ï¼š\n"
        elif func_radio == 'å•å…ƒæµ‹è¯•':
            prompt = "è¯·å¸®æˆ‘ç”Ÿæˆå¦‚ä¸‹" + lang_radio + "ä»£ç çš„å•å…ƒæµ‹è¯•ï¼Œä»£ç æ˜¯ï¼š\n"
        elif func_radio == 'æ·»åŠ æ³¨é‡Š':
            prompt = "è¯·å¸®æˆ‘ä¸ºå¦‚ä¸‹" + lang_radio + "ä»£ç æ·»åŠ æ³¨é‡Šï¼Œä»£ç æ˜¯ï¼š\n"
        program_request = st.text_area(label='ğŸ” é€‰æ‹©å“ªç§ç¨‹åºè¯­è¨€åï¼Œè¯·å†è¾“å…¥æ‚¨çš„è¯¦ç»†éœ€æ±‚ï¼š',
                                       placeholder='æ­¤å¤„è¾“å…¥æ‚¨çš„è¯¦ç»†éœ€æ±‚',
                                       value=prompt,
                                       height=200,
                                       key='program_prompt')
        ask = st.button('æé—®')
        if ask and program_request != '':
            last_program_prompt = st.session_state['chatgpt_program_prompt']
            # st.session_state['program_prompt'] = program_request
            generate_from_GPT4(program_request)

        if 'chatgpt_program_answer' in st.session_state and st.session_state['chatgpt_program_answer'] != '':
            answer = st.session_state['chatgpt_program_answer']
            st.text("ğŸ’¡ GPTçš„å›ç­”å¦‚ä¸‹ï¼š")
            color = RANK_COLOR[0]
            st.markdown(f":{color}[{answer}]")
