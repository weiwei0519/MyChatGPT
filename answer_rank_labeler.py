# coding=UTF-8
# Rank List æ ‡æ³¨å¹³å°ï¼Œç”¨äºæ ‡æ³¨ Reward Model çš„è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡streamlitæ­å»ºã€‚

'''
@File: answer_rank_labeler
@Author: WeiWei
@Time: 2023/3/5
@Email: weiwei_519@outlook.com
@Software: PyCharm
'''

import os

import streamlit as st
import torch
import json
from PIL import Image
from transformers import AutoTokenizer, GPT2LMHeadModel, TextGenerationPipeline
from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config
from model.T5_model import infer_answer
# from streamlit_autorefresh import st_autorefresh

st.set_page_config(
    page_title="AIA-GPT ç®¡ç†å¹³å°",
    page_icon="ğŸ¦ˆ",
    layout="wide"
)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

MODEL_CONFIG = {
    'model_name': './models/ChatYuan-large-v1',  # backbone
    'dataset_file': './datasets/company_datasets/human_rank_pairs.json',  # æ ‡æ³¨æ•°æ®é›†çš„å­˜æ”¾æ–‡ä»¶
    'rank_list_len': 3,  # æ’åºåˆ—è¡¨çš„é•¿åº¦
    'max_gen_seq_len': 512,  # ç”Ÿæˆç­”æ¡ˆæœ€å¤§é•¿åº¦
}

######################## é¡µé¢é…ç½®åˆå§‹åŒ– ###########################
RANK_COLOR = [
    'red',
    'green',
    'blue',
    'orange',
    'violet'
]

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š
# tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['model_name'])
# model = GPT2LMHeadModel.from_pretrained(MODEL_CONFIG['model_name'])
# TextGenerationPipeline(model, tokenizer, device=device)
# model.to(device)

# åŠ è½½Companyé¢„è®­ç»ƒæ¨¡å‹
tokenizer = T5Tokenizer.from_pretrained(MODEL_CONFIG['model_name'])
config = T5Config.from_pretrained(MODEL_CONFIG['model_name'])
model = T5ForConditionalGeneration.from_pretrained(MODEL_CONFIG['model_name'])
model.to(device)

######################## ä¼šè¯ç¼“å­˜åˆå§‹åŒ– ###########################
print('request & response!')

if 'prompt' not in st.session_state:
    st.session_state['prompt'] = ''

if 'query' not in st.session_state:
    st.session_state['query'] = ''

if 'answer' not in st.session_state:
    st.session_state['answer'] = ''

if 'answer_to_rank' not in st.session_state:
    answer_to_rank = {}
    st.session_state['answer_to_rank'] = answer_to_rank

if 'rank_choices' not in st.session_state:
    rank_choices = ['-1']
    for i in range(MODEL_CONFIG['rank_list_len']):
        rank_choices.append(str(i + 1))
    st.session_state['rank_choices'] = rank_choices


######################### å‡½æ•°å®šä¹‰åŒº ##############################
def generate_text(prompt, do_sample):
    """
    æ¨¡å‹ç”Ÿæˆæ–‡å­—ã€‚
    """
    print(f'prompt: {prompt}')
    # current_results = []
    # for _ in range(MODEL_CONFIG['rank_list_len']):
    #     if st.session_state['current_prompt'] != '':
    #         res = st.session_state['generator'](
    #             st.session_state['current_prompt'],
    #             max_length=MODEL_CONFIG['max_gen_seq_len'],
    #             do_sample=True,
    #             early_stopping=True,  # ä¿è¯å½“é‡åˆ°EOSè¯æ—¶ï¼Œç»“æŸç”Ÿæˆ
    #         )
    #         current_results.extend([e['generated_text'] for e in res])
    # st.session_state['current_results'] = current_results
    return_seqs = MODEL_CONFIG['rank_list_len']
    if do_sample:
        # æ ‡æ³¨æ¨¡å¼
        action = 'æ ‡æ³¨æ¨¡å¼'
        return_seqs = MODEL_CONFIG['rank_list_len']
    else:
        # é—®ç­”æ¨¡å¼
        action = 'é—®ç­”æ¨¡å¼'
        return_seqs = 1
    answers = infer_answer(text=prompt,
                           tokenizer=tokenizer,
                           model=model,
                           # do_sample=do_sample,
                           samples=return_seqs
                           )
    print(f'answer in generator: {len(answers)} x {len(answers[0])}')
    if action == 'æ ‡æ³¨æ¨¡å¼':
        st.session_state['answer_to_rank'] = {}
        for _, answer in enumerate(answers):
            st.session_state['answer_to_rank'][answer] = '-1'  # -1 ä¸ºåˆå§‹rankï¼Œç­‰äºæœªæ’åº
    elif action == 'é—®ç­”æ¨¡å¼':
        st.session_state['answer'] = answers[0]


######################### é¡µé¢å®šä¹‰åŒºï¼ˆä¾§è¾¹æ ï¼‰ ########################
with st.sidebar:
    st.image(Image.open('./images/AIA_logo.png'))
    st.sidebar.title('ğŸ“Œ AIA-GPT ç®¡ç†å¹³å°')
    st.sidebar.markdown('''
        ```python
        æä¾›ç”Ÿæˆå¼æé—®æµ‹è¯•ï¼Œä»¥åŠå†…å®¹çš„æ’åºæ ‡æ³¨
        
        Label Tabï¼šæ ‡æ³¨æ¨¡å¼ã€‚
            ç”¨æˆ·è¾“å…¥æç¤ºé—®é¢˜ï¼Œç³»ç»Ÿè¾“å‡ºå¤šä¸ªç­”æ¡ˆã€‚
            ç”¨æˆ·æ ¹æ®ç­”æ¡ˆçš„å‡†ç¡®æ€§è¿›è¡Œæ’åºã€‚
            
        Prompt Tabï¼šé—®ç­”æ¨¡å¼ã€‚
            ç”¨æˆ·è¾“å…¥æç¤ºé—®é¢˜ï¼Œç³»ç»Ÿè¾“å‡ºå”¯ä¸€ç­”æ¡ˆã€‚
            
        ```
    ''')

label_tab, prompt_tab = st.tabs(['Label', 'Prompt'])
answer_to_rank = []

######################### é¡µé¢å®šä¹‰åŒºï¼ˆæ ‡æ³¨é¡µé¢ï¼‰ ########################
with label_tab:
    with st.expander('AIA-GPTå¯¹è¯é—®ç­”ä¸“åŒº', expanded=True):  # expanded æ˜¯å±•å¼€è¿˜æ˜¯æ”¶ç¼©çŠ¶æ€
        label_query_txt = st.text_area('ğŸ” è¯·è¾“å…¥æ‚¨çš„æé—®ï¼š', placeholder='æ­¤å¤„è¾“å…¥æ‚¨çš„æé—®', key='label_query')
        if label_query_txt != '' and st.session_state['query'] != label_query_txt:
            st.session_state['query'] = label_query_txt
            generate_text(st.session_state['query'], do_sample=True)

    with st.expander('GPTçš„å›ç­”å¦‚ä¸‹ï¼š', expanded=True):
        answer_to_rank = st.session_state['answer_to_rank']
        if len(answer_to_rank) > 0:
            rank_choices = st.session_state['rank_choices']
            i = 0
            for answer, rank in answer_to_rank.items():
                if rank != '0':
                    rank_col, answer_col = st.columns([1, 10])
                    i += 1
                    curr_choice = int(rank) if rank != '-1' else 0
                    with rank_col:
                        st.text("ğŸ¥‡ Choose Rank")
                        choice = st.selectbox(f'å¥å­{i}æ’å', rank_choices,
                                              help='ä¸ºå½“å‰å¥å­é€‰æ‹©æ’åï¼Œæ’åè¶Šå°ï¼Œå¾—åˆ†è¶Šé«˜ã€‚ï¼ˆ-1ä»£è¡¨å½“å‰å¥å­æš‚æœªè®¾ç½®æ’åï¼‰')
                        if choice != curr_choice:
                            st.session_state['answer_to_rank'][answer] = choice
                            curr_choice = choice
                    with answer_col:
                        st.text("ğŸ’¡ Generated Answers")
                        if curr_choice != '-1':
                            color = RANK_COLOR[int(curr_choice) - 1]
                            st.markdown(f":{color}[{answer}]")
                        else:
                            color = 'white'
                            st.markdown(f":{color}[{answer}]")
                # st.session_state['answer_to_rank'] = answer_to_rank

            # æ‰‹å·¥å†™ç­”æ¡ˆ
            rank_col, answer_col = st.columns([1, 10])
            rank = MODEL_CONFIG['rank_list_len'] + 1
            with rank_col:
                st.text("ğŸ¥‡ Choose Rank")
                st.text('0')
            with answer_col:
                input_answer = st.text_area(label='ğŸ’¡ è¯·è¾“å…¥æ‚¨çš„ç­”æ¡ˆï¼š', placeholder='è¿™é‡Œæ‰‹å·¥è¾“å…¥ç­”æ¡ˆ')
                add_button = st.button('ï¼‹æ·»åŠ æ‰‹å·¥ç­”æ¡ˆ')
                if input_answer != '' and add_button:
                    st.session_state['answer_to_rank'][input_answer] = '0'
                    st.success('æˆåŠŸæ·»åŠ æ‰‹å·¥ç­”æ¡ˆï¼Œè¯·ç‚¹å‡»æŒ‰é’®å­˜å‚¨å½“å‰æ’åº', icon="âœ…")
                    # color = RANK_COLOR[0]
                    # st.markdown(f":{color}[{input_answer}]")
                # st.session_state['answer_to_rank'] = answer_to_rank

        else:
            rank_col, answer_col = st.columns([1, 10])
            with rank_col:
                st.text("ğŸ¥‡ Choose Rank")
            with answer_col:
                st.text("ğŸ’¡ Generate Answers")

    answer_to_rank_new = st.session_state['answer_to_rank']
    print(f'answer_to_rank: {answer_to_rank_new}')

    save_button = st.button('å­˜å‚¨å½“å‰æ’åº')
    if save_button:
        answer_to_rank = st.session_state['answer_to_rank']
        rank_list = []
        for ans, rank in answer_to_rank.items():
            if rank == '-1':
                st.error('è¯·å®Œæˆæ’åºåå†å­˜å‚¨ï¼', icon='ğŸš¨')  # æ‰€æœ‰æ ‡æ³¨éƒ½å·²æ’åºï¼Œæ‰èƒ½ä¿å­˜
                st.stop()
            if rank in rank_list:
                st.error('åºå·é€‰æ‹©é‡å¤ï¼', icon='ğŸš¨')  # åºå·é€‰æ‹©é‡å¤
                st.stop()
            else:
                rank_list.append(rank)

        # with open(MODEL_CONFIG['dataset_file'], 'a', encoding='utf8') as f:
        #     rank_texts = []
        #     for i in range(len(rank_results)):
        #         rank_texts.append(st.session_state['current_results'][rank_results.index(i + 1)])
        #     line = '\t'.join(rank_texts)
        #     f.write(f'{line}\n')
        file = open(MODEL_CONFIG['dataset_file'], 'r+', encoding='utf8')
        print(f'file information: {file}')
        content = file.read()
        print(f'orignal file content: {content}')
        file.seek(0)
        file.truncate()
        if len(content) != 0 and content != '':
            rank_pairs = json.loads(content)
        else:
            rank_pairs = {}
        length = len(rank_pairs)
        print(f'orignal json length: {length}')
        rank_pairs[length] = {}
        rank_pairs[length]['prompt'] = st.session_state['prompt'].strip()
        ranking = {}
        for ans, rank in answer_to_rank.items():
            ranking[rank] = ans
        ranked_answers = [ranking[key] for key in sorted(ranking.keys())]
        rank_pairs[length]['ranked_answers'] = ranked_answers
        # dumps()ï¼šå°†dictæ•°æ®è½¬åŒ–æˆjsonæ•°æ®ï¼›   dump()ï¼šå°†dictæ•°æ®è½¬åŒ–æˆjsonæ•°æ®åå†™å…¥jsonæ–‡ä»¶
        content = json.dumps(rank_pairs, ensure_ascii=False, indent=2)
        print(f'file save content {content}')
        # json.dump(content, file, ensure_ascii=False, indent=2)
        file.write(content)
        file.flush()
        st.success('ä¿å­˜æˆåŠŸï¼Œè¯·æ›´æ¢promptç”Ÿæˆæ–°çš„ç­”æ¡ˆ~', icon="âœ…")
        st.session_state.clear()
        # label_tab.empty()
        # st._rerun()
        # st_autorefresh(interval=2000, limit=100, key="fizzbuzzcounter")

######################### é¡µé¢å®šä¹‰åŒºï¼ˆé—®ç­”é¡µé¢ï¼‰ ########################
with prompt_tab:
    with st.expander('AIA-GPTå¯¹è¯é—®ç­”ä¸“åŒº', expanded=True):  # expanded æ˜¯å±•å¼€è¿˜æ˜¯æ”¶ç¼©çŠ¶æ€
        qa_query_txt = st.text_area('ğŸ” è¯·è¾“å…¥æ‚¨çš„æé—®ï¼š', placeholder='æ­¤å¤„è¾“å…¥æ‚¨çš„æé—®', key='qa_query')
        if qa_query_txt != '' and st.session_state['prompt'] != qa_query_txt:
            st.session_state['prompt'] = qa_query_txt
            generate_text(st.session_state['prompt'], do_sample=False)

    with st.expander('GPTçš„å›ç­”å¦‚ä¸‹ï¼š', expanded=True):
        if 'answer' in st.session_state:
            answer = st.session_state['answer']
            st.text("ğŸ’¡ Generated Answers")
            if answer != '':
                color = RANK_COLOR[0]
                st.markdown(f":{color}[{answer}]")
