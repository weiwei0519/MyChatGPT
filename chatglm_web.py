# coding=utf-8
# é¢å‘ITçš„å¼€å‘è¾…åŠ©å·¥å…·ã€‚åŸºäºchatglm
"""
# Project Name: MyChatGPT
# File Name: chatglm_web
# Author: NSNP577
# Creation at 2023/3/29 19:45
# IDE: PyCharm
# Describe: 
"""

import os
import streamlit as st
import torch
import json
from PIL import Image
import logging
import datetime
from transformers import AutoTokenizer, GPT2LMHeadModel, TextGenerationPipeline, AutoConfig, AutoModel
from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config
from model.T5_model import infer_answer
from steamship import Steamship
# from streamlit_autorefresh import st_autorefresh
import urllib3

urllib3.disable_warnings()

today = datetime.datetime.now().strftime("%Y-%m-%d")
logging.basicConfig(filename=f'./logs/labeler/labeler_{today}.log',
                    level=logging.INFO,
                    format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    filemode='a'
                    )

st.set_page_config(
    page_title="AIA-GPT ç®¡ç†å¹³å°",
    page_icon="ğŸ¦ˆ",
    layout="wide"
)

GPU = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
CPU = torch.device("cpu")

MODEL_CONFIG = {
    'model_AIA': './models/chatgpt-aia-chinese/ttt-aia-chinese',  # backbone
    'model_glm': './models/THUDM/chatglm-6b-int4',  # backbone
    'dataset_file': './datasets/company_datasets/human_rank_pairs.json',  # æ ‡æ³¨æ•°æ®é›†çš„å­˜æ”¾æ–‡ä»¶
    'rank_list_len': 3,  # æ’åºåˆ—è¡¨çš„é•¿åº¦
    'max_gen_seq_len': 400,  # ç”Ÿæˆç­”æ¡ˆæœ€å¤§é•¿åº¦
}

######################## é¡µé¢é…ç½®åˆå§‹åŒ– ###########################
RANK_COLOR = [
    'red',
    'green',
    'blue',
    'orange',
    'violet'
]

######################## ä¼šè¯ç¼“å­˜åˆå§‹åŒ– ###########################

if 'prompt' not in st.session_state:
    st.session_state['prompt'] = ''

if 'query' not in st.session_state:
    st.session_state['query'] = ''

if 'answer' not in st.session_state:
    st.session_state['answer'] = ''

if 'answer_to_rank' not in st.session_state:
    answer_to_rank = {}
    st.session_state['answer_to_rank'] = answer_to_rank

if 'rank_choices' not in st.session_state:
    rank_choices = ['-1']
    for i in range(MODEL_CONFIG['rank_list_len']):
        rank_choices.append(str(i + 1))
    st.session_state['rank_choices'] = rank_choices

# åŠ è½½GPT4 client
if 'GPT4_client' not in st.session_state:
    # client = Steamship(workspace="gpt-4", api_key='E7A97033-8208-452F-945D-782EA5E011AC', verify=False)
    # # create text generator from steamship client
    # generator = client.use_plugin('gpt-4')
    # st.session_state['GPT4_client'] = generator
    st.session_state['GPT4_client'] = ''

if 'program_prompt' not in st.session_state:
    st.session_state['program_prompt'] = ''

if 'program_answer' not in st.session_state:
    st.session_state['program_answer'] = ''


######################### å‡½æ•°å®šä¹‰åŒº ##############################
def generate_from_GPT4(prompt):
    logging.info(f'GPT4 prompt: {prompt}')
    generator = st.session_state['GPT4_client']
    task = generator.generate(text=prompt)
    task.wait()
    response = task.output.blocks[0].text
    # print(f'GPT4 answer is: {response}')
    st.session_state['program_answer'] = response


def generate_from_ChatGLM(prompt):
    logging.info('åŠ è½½ChatGLMé¢„è®­ç»ƒæ¨¡å‹')
    tokenizer_glm = AutoTokenizer.from_pretrained(MODEL_CONFIG['model_glm'], trust_remote_code=True)
    config_glm = AutoConfig.from_pretrained(MODEL_CONFIG['model_glm'], trust_remote_code=True)
    # model_glm = AutoModel.from_pretrained(MODEL_CONFIG['model_glm'],
    #                                       trust_remote_code=True,
    #                                       ignore_mismatched_sizes=True).float()  # CPU
    model_glm = AutoModel.from_pretrained(MODEL_CONFIG['model_glm'],
                                          trust_remote_code=True,
                                          ignore_mismatched_sizes=True).half().cuda()  # GPU
    logging.info('ChatGLMé¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ')
    logging.info(f'ChatGLM prompt: {prompt}')
    response, _ = model_glm.chat(tokenizer_glm, prompt, history=[])
    st.session_state['program_answer'] = response
    del model_glm, config_glm, tokenizer_glm
    torch.cuda.empty_cache()


######################### é¡µé¢å®šä¹‰åŒºï¼ˆä¾§è¾¹æ ï¼‰ ########################
with st.sidebar:
    st.image(Image.open('./images/AIA_logo.png'))
    st.sidebar.title('ğŸ“Œ AIA-GPT ç®¡ç†å¹³å°')
    st.sidebar.markdown('''
    ```python
    æä¾›åŸºäºGPTæ¨¡å‹çš„ç”Ÿæˆå¼å†…å®¹æé—®:

    Tab 1:
        Prompt Tabï¼šé—®ç­”æ¨¡å¼ã€‚
            ç”¨æˆ·è¾“å…¥æç¤ºé—®é¢˜ï¼Œç³»ç»Ÿè¾“å‡ºå”¯ä¸€ç­”æ¡ˆã€‚

    Tab 2:
        Label Tabï¼šæ ‡æ³¨æ¨¡å¼ã€‚
            ç”¨æˆ·è¾“å…¥æç¤ºé—®é¢˜ï¼Œç³»ç»Ÿè¾“å‡ºå¤šä¸ªç­”æ¡ˆã€‚
            ç”¨æˆ·æ ¹æ®ç­”æ¡ˆçš„å‡†ç¡®æ€§è¿›è¡Œæ’åºã€‚

    Tab 3:
        Programï¼šä»£ç è¾…åŠ©å·¥å…·
            åŸºäºéœ€æ±‚ç”Ÿæˆä»£ç ã€‚
            åŸºäºä»£ç ï¼Œæä¾›è§£è¯»ã€‚
            ä»£ç bugå‘ç°ä¸ä¿®å¤å»ºè®®ã€‚
            ä»£ç å•å…ƒæµ‹è¯•å»ºè®®ã€‚
            ä¸ºæ—¢æœ‰ä»£ç æ·»åŠ æ³¨é‡Šã€‚

    ```
    ''')

prompt_tab, label_tab, program_tab = st.tabs(['Program'])
answer_to_rank = []
st.write('<style>div.row-widget.stRadio > div{flex-direction:row;justify-content: left;} </style>',
         unsafe_allow_html=True)

# st.write('<style>div.st-bf{flex-direction:row;} div.st-ag{font-weight:bold;padding-left:2px;}</style>',
#          unsafe_allow_html=True)

######################### é¡µé¢å®šä¹‰åŒºï¼ˆä»£ç è¾…åŠ©å·¥å…·é¡µé¢ï¼‰ ########################
with program_tab:
    st.subheader(':blue[æä¾›é¢å‘ITå¼€å‘äººå‘˜çš„ä»£ç è¾…åŠ©å·¥å…·]')
    with st.expander('', expanded=True):  # expanded æ˜¯å±•å¼€è¿˜æ˜¯æ”¶ç¼©çŠ¶æ€
        func_options = ('è¯·é€‰æ‹©ï¼š', 'ç”Ÿæˆä»£ç ', 'ä»£ç è§£è¯»', 'ä¿®å¤bug', 'å•å…ƒæµ‹è¯•', 'æ·»åŠ æ³¨é‡Š')
        # gene_tab, read_tab, bug_tab, test_tab, comment_tab = st.tabs(func_options)
        func_radio = st.radio(label='è¯·é€‰æ‹©æ‚¨çš„éœ€æ±‚ï¼š',
                              options=func_options,
                              index=0,
                              key='func_options',
                              )
        lang_options = ('Java', 'SQL', 'Vue.js', 'Python', 'C++', 'Cobol', 'Swift')
        lang_radio = st.radio(label='è¯·é€‰æ‹©æ‚¨ä»£ç çš„è¯­è¨€ï¼š',
                              options=lang_options,
                              index=0,
                              key='lang_options_gene',
                              )
        prompt = ""
        if func_radio == 'ç”Ÿæˆä»£ç ':
            prompt = "è¯·å¸®æˆ‘ç”Ÿæˆä¸€æ®µ" + lang_radio + "ä»£ç ï¼Œéœ€æ±‚æ˜¯ï¼š\n"
        elif func_radio == 'ä»£ç è§£è¯»':
            prompt = "è¯·å¸®æˆ‘æ€»ç»“å¦‚ä¸‹" + lang_radio + "ä»£ç çš„ç”¨é€”ï¼Œä»£ç æ˜¯ï¼š\n"
        elif func_radio == 'ä¿®å¤bug':
            prompt = "è¯·å¸®æˆ‘å‘ç°å¹¶ä¿®å¤å¦‚ä¸‹" + lang_radio + "ä»£ç ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œä»£ç æ˜¯ï¼š\n"
        elif func_radio == 'å•å…ƒæµ‹è¯•':
            prompt = "è¯·å¸®æˆ‘ç”Ÿæˆå¦‚ä¸‹" + lang_radio + "ä»£ç çš„å•å…ƒæµ‹è¯•ï¼Œä»£ç æ˜¯ï¼š\n"
        elif func_radio == 'æ·»åŠ æ³¨é‡Š':
            prompt = "è¯·å¸®æˆ‘ä¸ºå¦‚ä¸‹" + lang_radio + "ä»£ç æ·»åŠ æ³¨é‡Šï¼Œä»£ç æ˜¯ï¼š\n"
        program_request = st.text_area(label='ğŸ” é€‰æ‹©å“ªç§ç¨‹åºè¯­è¨€åï¼Œè¯·å†è¾“å…¥æ‚¨çš„è¯¦ç»†éœ€æ±‚ï¼š',
                                       placeholder='æ­¤å¤„è¾“å…¥æ‚¨çš„è¯¦ç»†éœ€æ±‚',
                                       value=prompt,
                                       height=200,
                                       key='program_prompt')
        ask = st.button('æé—®')
        if ask and program_request != '':
            last_program_prompt = st.session_state['program_prompt']
            # st.session_state['program_prompt'] = program_request
            # generate_from_GPT4(program_request)
            st.session_state.disabled = True  # é˜²æ­¢é‡å¤æäº¤ï¼ŒèŠ‚çœå†…å­˜
            generate_from_ChatGLM(program_request)

        if 'program_answer' in st.session_state and st.session_state['program_answer'] != '':
            answer = st.session_state['program_answer']
            st.text("ğŸ’¡ GPTçš„å›ç­”å¦‚ä¸‹ï¼š")
            color = RANK_COLOR[0]
            st.markdown(f":{color}[{answer}]")

        st.session_state.disabled = False