# coding=utf-8
# T5 (Text-To-Text transfer transformer) model train
"""
# Project Name: MyGPT
# File Name: T5_model_train
# Author: NSNP577
# Creation at 2023/3/1 9:48
# IDE: PyCharm
# Describe: 
"""

import torch
from torch.utils.data import DataLoader, Dataset
import os
import time
import numpy as np
from transformers import T5Tokenizer, T5ForConditionalGeneration
from transformers import Trainer, TrainingArguments, get_linear_schedule_with_warmup
from torch_optimizer import Adafactor
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import pandas as pd
from utils.gpu_track import MemTracker
import inspect
import logging

# rich: for a better display on terminal
from rich.table import Column, Table
from rich import box
from rich.console import Console

# åšä¸€äº›ç›¸å…³çš„é…ç½®(æ‰“å°æ˜¾ç¤ºï¼›GPUè®¾ç½®)
# define a rich console logger
console = Console(record=True)
logging.basicConfig(level=logging.INFO)


def train(epoch, tokenizer, model, device, loader, optimizer):
    """
    ç”¨äºè®­ç»ƒçš„æ–¹æ³•
    Function to be called for training with the parameters passed from main function
    """
    # è¿½è¸ªGPU Memçš„æ¶ˆè€—æƒ…å†µã€‚
    frame = inspect.currentframe()  # define a frame to track
    gpu_tracker = MemTracker(frame)

    model.train()
    time1 = time.time()
    gpu_tracker.track()
    for _, data in enumerate(loader, 0):
        y = data["target_ids"].to(device, dtype=torch.long)
        y_ids = y[:, :-1].contiguous()
        lm_labels = y[:, 1:].clone().detach()  # target, for second to end.e.g."å¥½å—ï¼Ÿ"
        # releted to pad_token and loss. for detail, check here: https://github.com/Shivanandroy/T5-Finetuning-PyTorch/issues/3
        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100
        ids = data["source_ids"].to(device, dtype=torch.long)  # input. e.g. "how are you?"
        mask = data["source_mask"].to(device, dtype=torch.long)

        gpu_tracker.track()

        outputs = model(input_ids=ids, attention_mask=mask, labels=y, )
        loss = outputs[0]
        # æ¯100æ­¥æ‰“å°æ—¥å¿—
        if _ % 1 == 0 and _ != 0:
            time2 = time.time()
            print(_, "epoch:" + str(epoch) + "-loss:" + str(loss) + ";each step's time spent:" + str(
                float(time2 - time1) / float(_ + 0.0001)))
            # training_logger.add_row(str(epoch), str(_), str(loss))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


def validate(epoch, tokenizer, model, device, loader, max_length):
    """
    ç”¨äºéªŒè¯çš„æ–¹æ³•ï¼šè¾“å…¥ç”¨äºéªŒè¯çš„æ•°æ®ï¼Œè¿”å›æ¨¡å‹é¢„æµ‹çš„ç»“æœå’Œæ­£ç¡®çš„æ ‡ç­¾
    Function to evaluate model for predictions

    """
    model.eval()
    predictions = []
    actuals = []
    with torch.no_grad():
        for _, data in enumerate(loader, 0):
            y = data['target_ids'].to(device, dtype=torch.long)
            ids = data['source_ids'].to(device, dtype=torch.long)
            mask = data['source_mask'].to(device, dtype=torch.long)

            generated_ids = model.generate(
                input_ids=ids,
                attention_mask=mask,
                max_length=max_length,
                num_beams=2,
                repetition_penalty=2.5,
                length_penalty=1.0,
                early_stopping=True
            )
            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in
                     generated_ids]
            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]
            if _ % 1000 == 0:
                logging.debug(f'Completed {_}')

            predictions.extend(preds)
            actuals.extend(target)
    return predictions, actuals


def compute_metrics2(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }


def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = (labels == preds).sum() / len(labels)
    return {
        'accuracy': acc,
    }


def model_train(
        tokenizer, model, dataset, batch_size, epochs, learning_rate, device,
        model_dir="./models/CompanyModel0.1-TTT-Chinese",
        log_dir='./logs/TTT-train/',
        datasets_dir='./datasets/company_datasets/',
):
    # å‚æ•°å‚è€ƒè¿™ç¯‡æ–‡ç« ï¼šhttps://zhuanlan.zhihu.com/p/363670628
    training_args = TrainingArguments(
        output_dir=model_dir,  # output directory ç»“æœè¾“å‡ºåœ°å€
        num_train_epochs=epochs,  # total # of training epochs è®­ç»ƒæ€»æ‰¹æ¬¡
        per_device_train_batch_size=batch_size,  # batch size per device during training è®­ç»ƒæ‰¹å¤§å°
        per_device_eval_batch_size=batch_size,  # batch size for evaluation è¯„ä¼°æ‰¹å¤§å°
        evaluation_strategy="steps",  # Evaluation is done at the end of each epoch. or 10 steps
        logging_dir=log_dir,  # directory for storing logs æ—¥å¿—å­˜å‚¨ä½ç½®
        logging_strategy='epoch',
        learning_rate=learning_rate,  # å­¦ä¹ ç‡
        save_strategy='epoch',  # ä¸ä¿å­˜æ£€æŸ¥ç‚¹
        save_total_limit=1,  # åªä¿ç•™ä¸€ä¸ªcheckpoint
        overwrite_output_dir=True,  # è¦†ç›–ä¹‹å‰å†™çš„æ¨¡å‹è¾“å‡ºæ–‡ä»¶
        prediction_loss_only=True,  # åªè®¡ç®—lossä¸è®¡ç®—evaluation
        gradient_accumulation_steps=256 / batch_size,
        # æ˜¾å­˜é‡è®¡ç®—æ˜¯å…¸å‹çš„ç”¨æ—¶é—´æ¢ç©ºé—´ï¼Œæ¯”å¦‚æˆ‘ä»¬å¸Œæœ›è·‘256çš„å¤§ç‚¹çš„batchï¼Œä¸å¸Œæœ›è·‘32è¿™æ ·çš„å°batchï¼Œ
        # å› ä¸ºè§‰å¾—å°batchä¸ç¨³å®šï¼Œä¼šå½±å“æ¨¡å‹æ•ˆæœï¼Œä½†æ˜¯gpuæ˜¾å­˜åˆæ— æ³•æ”¾ä¸‹256çš„batchsizeçš„æ•°æ®ï¼Œ
        # æ­¤æ—¶æˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œæ˜¾å­˜é‡è®¡ç®—ï¼Œå°†è¿™ä¸ªå‚æ•°è®¾ç½®ä¸º256/32=8å³å¯ã€‚
        # ç”¨torchå®ç°å°±æ˜¯forwardï¼Œè®¡ç®—loss 8æ¬¡ï¼Œç„¶åå†optimizer.step()
    )

    optimizer = Adafactor(
        model.parameters(),
        lr=learning_rate,
        eps2=(1e-30, 1e-3),
        clip_threshold=1.0,
        decay_rate=-0.8,
        beta1=None,
        weight_decay=0.0,
        relative_step=False,
        scale_parameter=False,
        warmup_init=False
    )

    # å­¦ä¹ ç‡å˜åŒ–ç­–ç•¥
    total_steps = 0
    if len(dataset) % batch_size == 0:
        total_steps = (len(dataset) // batch_size) * epochs
    else:
        total_steps = (len(dataset) // batch_size + 1) * epochs
    warm_up_ratio = 0.1  # å®šä¹‰è¦é¢„çƒ­çš„step
    lr_scheduler = get_linear_schedule_with_warmup(optimizer,
                                                   num_warmup_steps=warm_up_ratio * total_steps,
                                                   num_training_steps=total_steps,
                                                   )
    trainer = Trainer(
        model=model,  # the instantiated ğŸ¤— Transformers model to be trained éœ€è¦è®­ç»ƒçš„æ¨¡å‹
        tokenizer=tokenizer,
        args=training_args,  # training arguments, defined above è®­ç»ƒå‚æ•°
        train_dataset=dataset,  # training dataset è®­ç»ƒé›†
        eval_dataset=dataset,  # evaluation dataset æµ‹è¯•é›†
        optimizers=(optimizer, lr_scheduler),  # è‡ªå®šä¹‰ä¼˜åŒ–å™¨
        compute_metrics=compute_metrics  # è®¡ç®—æŒ‡æ ‡æ–¹æ³•
    )

    # print(torch.cuda.memory_summary())

    trainer.train()


# to display dataframe in ASCII format
def display_df(df):
    """display dataframe in ASCII format"""

    # console = Console()
    table = Table(
        Column("source_text", justify="center"),
        Column("target_text", justify="center"),
        title="Sample Data",
        pad_edge=False,
        box=box.ASCII,
    )

    for i, row in enumerate(df.values.tolist()):
        table.add_row(row[0], row[1])

    logging.debug(table)  # TODO TODO TODO
