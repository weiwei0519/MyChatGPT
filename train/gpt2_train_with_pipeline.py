# coding=utf-8
# ä½¿ç”¨transformerè®­ç»ƒè‡ªå·±çš„æ¨¡å‹
"""
# Project Name: MyGPT
# File Name: MyGPT_train
# Author: NSNP577
# Creation at 2023/2/21 13:27
# IDE: PyCharm
# Describe: 
"""

import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Variable
import logging
import os
import random
import time
from tqdm import tqdm
from transformers import Trainer, TrainingArguments
from transformers import GPT2LMHeadModel, TextGenerationPipeline, GPT2Tokenizer, BertTokenizer
from transformers import WEIGHTS_NAME, CONFIG_NAME, GPT2Config
from tokenizers import Tokenizer
from tokenizers.models import BPE
import docx2txt

# device : GPU or CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
logging.basicConfig(level=logging.INFO)

# è®­ç»ƒå‚æ•°
batch_size = 256
epochs = 5000
learning_rate = 1e-5  # å­¦ä¹ ç‡
max_len = 512
action = 'validate'  # train è®­ç»ƒ   validate æµ‹è¯•  prod  ç”Ÿäº§è¿è¡Œ
model_dir = "./models/CompanyModel0.1-GPT2-Chinese/"


# ä½†è¿™æ ·æœ‰æ—¶å¯èƒ½ä¼šå‡ºç°é—®é¢˜ï¼Œä¾‹å¦‚æ¨¡å‹é™·å…¥ä¸€ä¸ªå¾ªç¯ï¼Œä¸æ–­ç”ŸæˆåŒä¸€ä¸ªå•è¯ã€‚
# ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œ GPT-2 è®¾ç½®äº†ä¸€ä¸ª top-k å‚æ•°ï¼Œè¿™æ ·æ¨¡å‹å°±ä¼šä»æ¦‚ç‡å‰ k å¤§çš„å•è¯ä¸­éšæœºé€‰å–ä¸€ä¸ªå•è¯ï¼Œä½œä¸ºä¸‹ä¸€ä¸ªå•è¯ã€‚
def select_top_k(predictions, k=10):
    predicted_tokens = random.choice(
        predictions[0, -1, :].sort(descending=True)[1][:10]).item()
    return predicted_tokens

'''
modelç±»æ˜¯ç›®å‰åœ¨åº“ä¸­æä¾›çš„8ä¸ªæ¨¡å‹æ¶æ„çš„PyTorchæ¨¡å‹(torch.nn.Modules)ï¼Œä¾‹å¦‚BertModel
configurationç±»ï¼Œå®ƒå­˜å‚¨æ„å»ºæ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰å‚æ•°ï¼Œä¾‹å¦‚BertConfigã€‚æ‚¨ä¸å¿…æ€»æ˜¯è‡ªå·±å®ä¾‹åŒ–è¿™äº›é…ç½®ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯æœªç»ä»»ä½•ä¿®æ”¹çš„é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œåˆ›å»ºæ¨¡å‹å°†è‡ªåŠ¨è´Ÿè´£å®ä¾‹åŒ–é…ç½®(å®ƒæ˜¯æ¨¡å‹çš„ä¸€éƒ¨åˆ†)
tokenizerç±»ï¼Œå®ƒå­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„è¯æ±‡è¡¨ï¼Œå¹¶åœ¨è¦è¾“é€åˆ°æ¨¡å‹çš„è¯æ±‡åµŒå…¥ç´¢å¼•åˆ—è¡¨ä¸­æä¾›ç”¨äºç¼–ç /è§£ç å­—ç¬¦ä¸²çš„æ–¹æ³•ï¼Œä¾‹å¦‚BertTokenizer
from_pretraining()å…è®¸æ‚¨ä»ä¸€ä¸ªé¢„è®­ç»ƒç‰ˆæœ¬å®ä¾‹åŒ–ä¸€ä¸ªæ¨¡å‹/é…ç½®/tokenizer
save_pretraining()å…è®¸æ‚¨åœ¨æœ¬åœ°ä¿å­˜æ¨¡å‹/é…ç½®/tokenizer
'''
'''
config = GPT2Config(
    architectures=["GPT2LMHeadModel"],   # pretrainçš„æ—¶å€™ç”¨æ¥é¢„åŠ è½½æ¨¡å‹
    model_type="GPT2LMHeadModel",        # å®šä¹‰æ¨¡å‹ç±»å‹ï¼Œå¯¼å‡ºç»™`AutoConfig`ç”¨ï¼Œå¦‚æœè¦ä¸Šä¼ åˆ°hubè¯·å¿…å¡«
    tokenizer_class="GPT2Tokenizer",       # å®šä¹‰tokenizerç±»å‹ï¼Œå¯¼å‡ºç»™`AutoTokenizer`ç”¨ï¼Œå¦‚æœè¦ä¸Šä¼ åˆ°hubè¯·å¿…å¡«
    vocab_size=8021,
    n_positions=1024,
    n_ctx=1024,
    n_embd=768,
    n_layer=6,
    n_head=6,
    pad_token_id=tokenizer.pad_token_id,   # å‰é¢æ„å»ºçš„tokenizerçš„ PAD ID
    task_specific_params={
        "text-generation": {
            "do_sample": True,
            "max_length": 120
        }
    }
)
'''
def train():
    # åˆå§‹åŒ–transformeræ¨¡å‹(ç©ºæ¨¡å‹)
    # tokenizer = Tokenizer.from_pretrained('./models/CompanyModel0.1-GPT2-Chinese')
    tokenizer = Tokenizer(BPE())
    tokenizer.model = BPE.from_file('./models/CompanyModel0.1-GPT2-Chinese/vocab.json',
                                    './models/CompanyModel0.1-GPT2-Chinese/merges.txt')
    print('initial pre-trained Tokenizer')
    config = GPT2Config(
        architectures=["GPT2LMHeadModel"],   # pretrainçš„æ—¶å€™ç”¨æ¥é¢„åŠ è½½æ¨¡å‹
        model_type="GPT2LMHeadModel",        # å®šä¹‰æ¨¡å‹ç±»å‹ï¼Œå¯¼å‡ºç»™`AutoConfig`ç”¨ï¼Œå¦‚æœè¦ä¸Šä¼ åˆ°hubè¯·å¿…å¡«
        tokenizer_class="Tokenizer",       # å®šä¹‰tokenizerç±»å‹ï¼Œå¯¼å‡ºç»™`AutoTokenizer`ç”¨ï¼Œå¦‚æœè¦ä¸Šä¼ åˆ°hubè¯·å¿…å¡«
        vocab_size=8021,
        n_positions=1024,
        n_ctx=1024,
        n_embd=768,
        n_layer=6,
        n_head=6,
        pad_token_id=0,   # å‰é¢æ„å»ºçš„tokenizerçš„ PAD ID
        task_specific_params={
            "text-generation": {
                "do_sample": True,
                "max_length": 120
            }
        }
    )
    print('initial GPT2 Config')
    model = GPT2LMHeadModel(config=config)
    print('initial GPT2 Model, num parameters: = {0}'.format(model.num_parameters()))
    # model = GPT2LMHeadModel.from_pretrained('./models/gpt2-chinese-cluecorpussmall')
    # åŠ è½½ä¸­æ–‡GPT2æ¨¡å‹
    # tokenizer = GPT2Tokenizer.from_pretrained('./models/Wenzhong2.0-GPT2-3.5B-chinese')
    # model = GPT2LMHeadModel.from_pretrained('./models/Wenzhong2.0-GPT2-3.5B-chinese')
    text_generator = TextGenerationPipeline(model, tokenizer)
    print('initial GPT2 text generator')
    model.to(device)

    # æ•°æ®é›†
    # é€šè¿‡æ–°çš„æ–‡æœ¬é›†è®­ç»ƒè¿›è¡Œå¾®è°ƒ
    # with open('./datasets/generative_datasets/romeo_and_juliet.txt', 'r', encoding='UTF-8') as f:
    #     text = f.read()
    # print('datasets length = {0}'.format(len(text)))

    doc_path = './datasets/document/'
    files = os.listdir(doc_path)
    doc_texts = []
    for file in files:
        # f = open(doc_path + file, 'r', encoding='UTF-8')
        text = docx2txt.process(doc_path + file)
        doc_texts.append(text.replace("\n\n", "\n"))

    # é¢„å¤„ç†è®­ç»ƒé›†ï¼Œå°†è®­ç»ƒé›†ç¼–ç ã€åˆ†æ®µ
    dataset = []
    # æˆªå–solution
    # for i in range(len(text) // max_len):
    #     # å°†å­—ç¬¦ä¸²åˆ†æ®µæˆé•¿åº¦ä¸ºmax_lenä¸ºå•ä½
    #     dataset.extend(tokenizer.encode(text=text[i * max_len:(i + 1) * max_len]))
    # del text
    # ä¸æˆªå–solution
    for i in range(len(doc_texts)):
        # å°†å­—ç¬¦ä¸²åˆ†æ®µæˆé•¿åº¦ä¸ºmax_lenä¸ºå•ä½
        dataset.extend(tokenizer.encode(doc_texts[i]).ids)
    del text

    dataset_tensor = torch.tensor(dataset)
    print('datasets shape = {0}'.format(dataset_tensor.shape))

    # æ„å»ºæ•°æ®é›†å’Œæ•°æ®è¿­ä»£å™¨ï¼Œè®¾å®š batch_size å¤§å°ä¸º 2
    train_set = TensorDataset(dataset_tensor,
                              dataset_tensor)  # æ ‡ç­¾ä¸æ ·æœ¬æ•°æ®ç›¸åŒ
    train_loader = DataLoader(dataset=train_set,
                              batch_size=batch_size,
                              shuffle=False)

    # å¼€å§‹æ¨¡å‹è®­ç»ƒ
    pre = time.time()
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # å®šä¹‰ä¼˜åŒ–å™¨

    for i in range(epochs):
        total_loss = 0
        for batch_idx, (data, target) in enumerate(tqdm(train_loader)):
            data, target = Variable(data).to(device), Variable(target).to(device)
            # æ¯è½®epochä¹‹å‰ï¼Œå…ˆæ¸…é›¶æ¢¯åº¦
            optimizer.zero_grad()
            # è®¡ç®—è¾“å‡ºå’Œloss
            # loss, logits, _ = model(data, labels=target)
            output = model(data, labels=target)
            loss = output.loss
            logits = output.logits
            total_loss += loss.item()

            loss.backward()
            optimizer.step()

            if batch_idx == len(train_loader) - 1:
                # åœ¨æ¯ä¸ª Epoch çš„æœ€åè¾“å‡ºä¸€ä¸‹ç»“æœ
                print('epoch: {0}   average loss: {1}'.format(i, total_loss / len(train_loader)))

    #ä¿å­˜ç»è¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ã€é…ç½®å’Œè¯æ±‡è¡¨
    model_to_save = model.module if hasattr(model, 'module') else model
    # ä½¿ç”¨é¢„å®šä¹‰çš„åç§°ä¿å­˜ï¼Œåˆ™å¯ä»¥ä½¿ç”¨`from_pretrained`åŠ è½½
    output_model_file = os.path.join(model_dir, WEIGHTS_NAME)
    output_config_file = os.path.join(model_dir, CONFIG_NAME)

    torch.save(model_to_save.state_dict(), output_model_file)
    model_to_save.config.to_json_file(output_config_file)
    # tokenizer.save_vocabulary(model_dir)
    print('è®­ç»ƒæ—¶é—´ï¼š', time.time() - pre)

# æ¨¡å‹æµ‹è¯•
def validate(text):
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š
    model = GPT2LMHeadModel.from_pretrained('./models/CompanyModel0.1-GPT2-Chinese')
    tokenizer = Tokenizer(BPE())
    tokenizer.model = BPE.from_file('./models/CompanyModel0.1-GPT2-Chinese/vocab.json',
                                    './models/CompanyModel0.1-GPT2-Chinese/merges.txt')
    text_tokens = tokenizer.encode(text).ids
    tokens_tensor = torch.tensor([text_tokens])
    model.to(device)
    model.eval()
    total_predicted_text = text

    # ä½¿è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œ 500 æ¬¡é¢„æµ‹
    for _ in range(500):
        tokens_tensor = tokens_tensor.to(device)

        with torch.no_grad():
            outputs = model(tokens_tensor)
            predictions = outputs[0]

        predicted_tokens = select_top_k(predictions, k=10)

        predicted_text = tokenizer.decode(text_tokens + [predicted_tokens])
        total_predicted_text += tokenizer.decode([predicted_tokens])
        if '<|endoftext|>' in total_predicted_text:
            # å¦‚æœå‡ºç°æ–‡æœ¬ç»“æŸæ ‡å¿—ï¼Œå°±ç»“æŸæ–‡æœ¬ç”Ÿæˆ
            break

        text_tokens += [predicted_tokens]

        if len(text_tokens) > 1023:
            # æ¨¡å‹æœ€é•¿è¾“å…¥é•¿åº¦ä¸º1024ï¼Œå¦‚æœé•¿åº¦è¿‡é•¿åˆ™æˆªæ–­
            text_tokens = text_tokens[-1023:]

        tokens_tensor = torch.tensor([text_tokens])

    print(total_predicted_text)

# X = torch.zeros((26, 26), dtype=torch.float32).to(device=device)
# labels = []
# for i in range(26):
#     labels.append((i + 1) % 26)
#     X[i][i] = 1.
# labels = torch.tensor(labels)
# dataset = Dataset.from_dict({'x': X, 'labels': labels})
#
#
# # æ®‹å·®ç½‘ç»œ
# class RN(nn.Module):
#     def __init__(self):
#         super(RN, self).__init__()
#         self.linear_stack = nn.Sequential(
#             nn.Linear(26, 64),
#             nn.Hardsigmoid(),
#             nn.Linear(64, 26),
#             nn.Hardsigmoid(),
#         )
#
#         self.linear_stack_2 = nn.Sequential(
#             nn.Linear(26, 64),
#             nn.Hardsigmoid(),
#             nn.Linear(64, 64),
#             nn.Hardsigmoid(),
#         )
#
#         self.output_layer = nn.Linear(64, 26)
#
#         self.loss_f = nn.CrossEntropyLoss()
#
#     def forward(self, x, labels, mode='train'):
#         y = self.linear_stack(x)
#         # æ®‹å·®
#         y = y + x
#         y = self.linear_stack_2(y)
#         y = self.output_layer(y)
#
#         if mode == 'train':
#             return {
#                 'loss': self.loss_f(y, labels),
#                 'predictions': y
#             }
#
#         return y
#
#
# # ç”Ÿæˆæ¨¡å‹å®ä¾‹
# model = RN().to(device=device)
#
#
# def compute_metrics(pred):
#     labels = pred.label_ids
#     preds = pred.predictions.argmax(-1)
#     acc = (labels == preds).sum() / len(labels)
#     return {
#         'accuracy': acc,
#     }
#
#
# training_args = TrainingArguments(
#     output_dir='./models/CompanyModel0.1-GPT2-Chinese',  # output directory ç»“æœè¾“å‡ºåœ°å€
#     num_train_epochs=epochs,  # total # of training epochs è®­ç»ƒæ€»æ‰¹æ¬¡
#     per_device_train_batch_size=batch_size,  # batch size per device during training è®­ç»ƒæ‰¹å¤§å°
#     per_device_eval_batch_size=batch_size,  # batch size for evaluation è¯„ä¼°æ‰¹å¤§å°
#     logging_dir='./logs/rn_log',  # directory for storing logs æ—¥å¿—å­˜å‚¨ä½ç½®
#     learning_rate=learning_rate,  # å­¦ä¹ ç‡
#     save_steps=False,  # ä¸ä¿å­˜æ£€æŸ¥ç‚¹
# )
#
# trainer = Trainer(
#     model=model,  # the instantiated ğŸ¤— Transformers model to be trained éœ€è¦è®­ç»ƒçš„æ¨¡å‹
#     args=training_args,  # training arguments, defined above è®­ç»ƒå‚æ•°
#     train_dataset=train_set,  # training dataset è®­ç»ƒé›†
#     eval_dataset=train_set,  # evaluation dataset æµ‹è¯•é›†
#     compute_metrics=compute_metrics  # è®¡ç®—æŒ‡æ ‡æ–¹æ³•
# )
#
# trainer.train()
# trainer.evaluate()

if __name__ == '__main__':
    if action == 'train':
        train()
    elif action == 'validate':
        cont = True
        while cont:
            input_text = str(input("è¯·è¾“å…¥/Please inputï¼š "))

            if input_text == "exit":
                cont = False
            else:
                validate(input_text)
