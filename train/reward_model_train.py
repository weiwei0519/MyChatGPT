# !/usr/bin/env python3
# ä½¿ç”¨ChatGPTä¸­Reward Modelçš„æ€è·¯è®­ç»ƒä¸€ä¸ªRMï¼Œå› ä¸ºæ˜¯æ‰“åˆ†æ¨¡åž‹ï¼Œæ‰€ä»¥ä½¿ç”¨BERTï¼ˆè€ŒéžGPTï¼‰æ¨¡åž‹è®­ç»ƒã€‚

"""
# Project Name: MyChatGPT
# File Name: training_logger
# Author: NSNP577
# Creation at 2023/3/2 11:42
# IDE: PyCharm
# Describe:
"""
import os
import io
import time
import argparse
import logging
from functools import partial
from tqdm import tqdm
from rich import print
import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel, default_data_collator, get_scheduler, GPT2LMHeadModel
from transformers import get_linear_schedule_with_warmup
from transformers import TrainingArguments, DataCollatorWithPadding
from reward_model import RewardModel, RewardModelTrainer
from utils.dataset_util import dataset_process, TextRewardDataset
from utils.training_logger import LoggerDisplayer
from torch_optimizer import Adafactor

# device : GPU or CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device = 'cpu'
print(device)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"  # é˜²æ­¢GPUå†…å­˜æº¢å‡º

parser = argparse.ArgumentParser()
parser.add_argument("--model", default="../models/bert-base-chinese", type=str, help="backbone of encoder.")
parser.add_argument("--train_path", default="../datasets/reward_model_dataset/train.tsv", type=str,
                    help="The path of train set.")
parser.add_argument("--val_path", default="../datasets/reward_model_dataset/val.tsv", type=str,
                    help="The path of dev set.")
parser.add_argument("--save_dir", default="../models/CompanyModel0.1-RewardModel-Chinese", type=str, required=False,
                    help="The output directory where the model predictions and checkpoints will be written.")
parser.add_argument("--max_seq_len", default=512, type=int,
                    help="The maximum total input sequence length after tokenization. Sequences longer "
                         "than this will be truncated, sequences shorter will be padded.", )
parser.add_argument("--batch_size", default=2, type=int, help="Batch size per GPU/CPU for training.", )
parser.add_argument("--learning_rate", default=5e-5, type=float, help="The initial learning rate for Adam.")
parser.add_argument("--weight_decay", default=0.0, type=float, help="Weight decay if we apply some.")
parser.add_argument("--epochs", default=100, type=int, help="Total number of training epochs to perform.")
parser.add_argument("--warmup_ratio", default=0.0, type=float, help="Linear warmup over warmup_ratio * total_steps.")
parser.add_argument("--valid_steps", default=200, type=int, required=False, help="evaluate frequecny.")
parser.add_argument("--logging_steps", default=10, type=int, help="log interval.")
parser.add_argument("--img_log_dir", default='./logs/reward_model_train', type=str, help="Logging image path.")
parser.add_argument("--img_log_name", default='reward_model.log', type=str, help="Logging image file name.")
args = parser.parse_args()

# logger_displayer = LoggerDisplayer(log_path=args.img_log_dir, log_name=args.img_log_name)
logging.basicConfig(level=logging.INFO)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = (labels == preds).sum() / len(labels)
    return {
        'accuracy': acc,
    }


def evaluate_model(model, data_loader):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å½“å‰æ¨¡åž‹çš„è®­ç»ƒæ•ˆæžœã€‚

    Args:
        model: å½“å‰æ¨¡åž‹
        data_loader: æµ‹è¯•é›†çš„dataloader
    """
    model.eval()
    with torch.no_grad():
        batch_rank_rewards = []
        for batch in data_loader:
            for batch_idx in range(len(batch['input_ids'])):
                rank_texts_count = len(batch['input_ids'][batch_idx])
                rank_rewards = []
                for text_idx in range(rank_texts_count):
                    reward = model(
                        batch['input_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
                        batch['token_type_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
                        batch['attention_mask'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
                        batch['position_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
                    )
                    rank_rewards.append(reward[0])  # (rank_text_num) -> [tensor([0.1696]), tensor([0.3466])]
                batch_rank_rewards.append(
                    rank_rewards)  # (batch, rank_text_num) -> [[tensor([0.1696]), tensor([0.3466])], ...]
    model.train()
    total_ranklist, right_ranklist = 0, 0
    for rank_rewards in batch_rank_rewards:
        rank_rewards = [t.cpu().float() for t in rank_rewards]
        rank_rewards_sorted = sorted(rank_rewards, reverse=True)
        total_ranklist += 1
        if rank_rewards_sorted == rank_rewards:
            right_ranklist += 1
    return right_ranklist / total_ranklist


def train():
    encoder = AutoModel.from_pretrained(args.model)
    model = RewardModel(encoder=encoder)
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model_size = sum(t.numel() for t in model.parameters())
    print(f"model size: {model_size / 1000 ** 2:.1f}M parameters")
    encoder.to(device)
    model.to(device)
    # dataset = load_dataset('text', data_files={'train': args.train_path,
    #                                            'val': args.dev_path})
    # print(dataset)
    # # dataset = dataset_process(dataset=dataset, tokenizer=tokenizer, max_seq_len=args.max_seq_len)
    # convert_func = partial(dataset_process, tokenizer=tokenizer, max_seq_len=args.max_seq_len)
    # dataset = dataset.map(convert_func)  # batched=Trueé‡‡ç”¨æ‰¹é‡å¤„ç†çš„æ–¹å¼ï¼Œæå‡performance
    #
    # train_dataset = dataset["train"]
    # eval_dataset = dataset["val"]
    # åŠ è½½è®­ç»ƒé›†
    texts = io.open(args.train_path, encoding='UTF-8').read().strip().split('\n')
    train_dataset = TextRewardDataset(texts=texts, tokenizer=tokenizer, max_len=args.max_seq_len)
    # åŠ è½½æµ‹è¯•é›†
    texts = io.open(args.val_path, encoding='UTF-8').read().strip().split('\n')
    val_dataset = TextRewardDataset(texts=texts, tokenizer=tokenizer, max_len=args.max_seq_len)
    train_dataloader = DataLoader(train_dataset, shuffle=False, collate_fn=default_data_collator,
                                  batch_size=args.batch_size)
    eval_dataloader = DataLoader(val_dataset, shuffle=False, collate_fn=default_data_collator,
                                 batch_size=args.batch_size)

    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": args.weight_decay,
        },
        {
            "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
    ]
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

    # æ ¹æ®è®­ç»ƒè½®æ•°è®¡ç®—æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œä»¥ä¾¿äºŽscheduleråŠ¨æ€è°ƒæ•´lr
    num_update_steps_per_epoch = len(train_dataloader)
    max_train_steps = args.epochs * num_update_steps_per_epoch
    warm_steps = int(args.warmup_ratio * max_train_steps)
    lr_scheduler = get_scheduler(
        name='linear',
        optimizer=optimizer,
        num_warmup_steps=warm_steps,
        num_training_steps=max_train_steps,
    )

    training_args = TrainingArguments(
        output_dir=args.save_dir,  # output directory ç»“æžœè¾“å‡ºåœ°å€
        num_train_epochs=args.epochs,  # total # of training epochs è®­ç»ƒæ€»æ‰¹æ¬¡
        per_device_train_batch_size=args.batch_size,  # batch size per device during training è®­ç»ƒæ‰¹å¤§å°
        per_device_eval_batch_size=args.batch_size,  # batch size for evaluation è¯„ä¼°æ‰¹å¤§å°
        evaluation_strategy="steps",  # Evaluation is done at the end of each epoch. or 10 steps
        logging_dir=args.img_log_dir,  # directory for storing logs æ—¥å¿—å­˜å‚¨ä½ç½®
        logging_strategy='epoch',
        learning_rate=args.learning_rate,  # å­¦ä¹ çŽ‡
        save_strategy='epoch',  # ä¸ä¿å­˜æ£€æŸ¥ç‚¹
        save_total_limit=1,  # åªä¿ç•™ä¸€ä¸ªcheckpoint
        overwrite_output_dir=True,  # è¦†ç›–ä¹‹å‰å†™çš„æ¨¡åž‹è¾“å‡ºæ–‡ä»¶
        prediction_loss_only=True,  # åªè®¡ç®—lossä¸è®¡ç®—evaluation
        gradient_accumulation_steps=256 / args.batch_size,
        # æ˜¾å­˜é‡è®¡ç®—æ˜¯å…¸åž‹çš„ç”¨æ—¶é—´æ¢ç©ºé—´ï¼Œæ¯”å¦‚æˆ‘ä»¬å¸Œæœ›è·‘256çš„å¤§ç‚¹çš„batchï¼Œä¸å¸Œæœ›è·‘32è¿™æ ·çš„å°batchï¼Œ
        # å› ä¸ºè§‰å¾—å°batchä¸ç¨³å®šï¼Œä¼šå½±å“æ¨¡åž‹æ•ˆæžœï¼Œä½†æ˜¯gpuæ˜¾å­˜åˆæ— æ³•æ”¾ä¸‹256çš„batchsizeçš„æ•°æ®ï¼Œ
        # æ­¤æ—¶æˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œæ˜¾å­˜é‡è®¡ç®—ï¼Œå°†è¿™ä¸ªå‚æ•°è®¾ç½®ä¸º256/32=8å³å¯ã€‚
        # ç”¨torchå®žçŽ°å°±æ˜¯forwardï¼Œè®¡ç®—loss 8æ¬¡ï¼Œç„¶åŽå†optimizer.step()
    )

    # callè‡ªå®šä¹‰Trainerï¼Œé‡å†™äº†compute_loss
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    trainer = RewardModelTrainer(
        model=model,  # the instantiated ðŸ¤— Transformers model to be trained éœ€è¦è®­ç»ƒçš„æ¨¡åž‹
        tokenizer=tokenizer,
        args=training_args,  # training arguments, defined above è®­ç»ƒå‚æ•°
        train_dataset=train_dataset,  # training dataset è®­ç»ƒé›†
        eval_dataset=val_dataset,  # evaluation dataset æµ‹è¯•é›†
        optimizers=(optimizer, lr_scheduler),  # è‡ªå®šä¹‰ä¼˜åŒ–å™¨
        data_collator=data_collator,  # ä½¿ç”¨åŠ¨æ€paddingï¼ŒèŠ‚çœè®­ç»ƒå†…å­˜å ç”¨
        compute_metrics=compute_metrics  # è®¡ç®—æŒ‡æ ‡æ–¹æ³•
    )

    # print(torch.cuda.memory_summary())
    trainer.train()

    # loss_list = []
    # tic_train = time.time()
    # global_step, best_acc = 0, 0
    # for epoch in range(1, args.epochs + 1):
    #     print(f"epoch: {epoch}")
    #     for _, batch in enumerate(tqdm(train_dataloader)):
    #         batch_rank_rewards = []
    #         for batch_idx in range(len(batch['input_ids'])):
    #             rank_texts_count = len(batch['input_ids'][batch_idx])
    #             rank_rewards = []
    #             for text_idx in range(rank_texts_count):
    #                 reward = model(
    #                     batch['input_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
    #                     batch['token_type_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
    #                     batch['attention_mask'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
    #                     batch['position_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
    #                 )
    #                 rank_rewards.append(reward[0])  # (rank_text_num) -> [tensor([0.1696]), tensor([0.3466])]
    #             batch_rank_rewards.append(
    #                 rank_rewards)  # (batch, rank_text_num) -> [[tensor([0.1696]), tensor([0.3466])], ...]
    #         loss = compute_rank_list_loss(batch_rank_rewards)
    #         loss.backward()
    #         optimizer.step()
    #         lr_scheduler.step()
    #         optimizer.zero_grad()
    #         loss_list.append(float(loss.cpu().detach()))
    #
    #         global_step += 1
    #         if global_step % args.logging_steps == 0:
    #             time_diff = time.time() - tic_train
    #             loss_avg = sum(loss_list) / len(loss_list)
    #             logger_displayer.add_scalar('train/train_loss', loss_avg, global_step)
    #             print("global step %d, epoch: %d, loss: %.5f, speed: %.2f step/s"
    #                   % (global_step, epoch, loss_avg, args.logging_steps / time_diff))
    #             tic_train = time.time()
    #
    #         if global_step % args.valid_steps == 0:
    #             cur_save_dir = os.path.join(args.save_dir, "model_%d" % global_step)
    #             if not os.path.exists(cur_save_dir):
    #                 os.makedirs(cur_save_dir)
    #             torch.save(model, os.path.join(cur_save_dir, 'model.pt'))
    #             tokenizer.save_pretrained(cur_save_dir)
    #             acc = evaluate_model(model, eval_dataloader)
    #             logger_displayer.add_scalar('eval/accuracy', acc, global_step)
    #             logger_displayer.record()
    #             print("Evaluation acc: %.5f" % (acc))
    #             if acc > best_acc:
    #                 print(
    #                     f"best F1 performence has been updated: {best_acc:.5f} --> {acc:.5f}"
    #                 )
    #                 best_acc = acc
    #                 cur_save_dir = os.path.join(args.save_dir, "model_best")
    #                 if not os.path.exists(cur_save_dir):
    #                     os.makedirs(cur_save_dir)
    #                 torch.save(model, os.path.join(cur_save_dir, 'model.pt'))
    #                 tokenizer.save_pretrained(cur_save_dir)
    #             tic_train = time.time()


if __name__ == '__main__':
    train()
