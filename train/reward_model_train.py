# !/usr/bin/env python3
# ä½¿ç”¨ChatGPTä¸­Reward Modelçš„æ€è·¯è®­ç»ƒä¸€ä¸ªRMï¼Œå› ä¸ºæ˜¯æ‰“åˆ†æ¨¡åž‹ï¼Œæ‰€ä»¥ä½¿ç”¨BERTï¼ˆè€ŒéžGPTï¼‰æ¨¡åž‹è®­ç»ƒã€‚

"""
# Project Name: MyChatGPT
# File Name: training_logger
# Author: NSNP577
# Creation at 2023/3/2 11:42
# IDE: PyCharm
# Describe:
"""
import os
import time, datetime
import argparse
import logging
import json
from tqdm import tqdm
from rich import print
import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModel, AutoConfig
from transformers import default_data_collator, get_scheduler, DataCollatorWithPadding
from transformers import TrainingArguments
from model.reward_model import RewardModel, compute_rank_list_loss, RewardModelTrainer
from utils.dataset_util import TextRewardDataset
from utils.training_logger import LoggerWriter
import shutil

# device : GPU or CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device = 'cpu'
print(device)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"  # é˜²æ­¢GPUå†…å­˜æº¢å‡º

parser = argparse.ArgumentParser()
parser.add_argument("--model", default="../models/bert-base-chinese", type=str,
                    help="backbone of encoder.")
parser.add_argument("--train_path", default="../datasets/company_dataset/human_rank_pairs.json",
                    type=str,
                    help="The path of train set.")
parser.add_argument("--val_path", default="../datasets/reward_model_dataset/human_rank_pairs.json", type=str,
                    help="The path of dev set.")
parser.add_argument("--save_dir", default="../models/chatgpt-aia-chinese/rm-aia-chinese", type=str, required=False,
                    help="The output directory where the model predictions and checkpoints will be written.")
parser.add_argument("--max_seq_len", default=512, type=int,
                    help="The maximum total input sequence length after tokenization. Sequences longer "
                         "than this will be truncated, sequences shorter will be padded.", )
parser.add_argument("--batch_size", default=2, type=int, help="Batch size per GPU/CPU for training.", )
parser.add_argument("--learning_rate", default=5e-4, type=float, help="The initial learning rate for Adam.")
parser.add_argument("--weight_decay", default=0.0, type=float, help="Weight decay if we apply some.")
parser.add_argument("--epochs", default=1000, type=int, help="Total number of training epochs to perform.")
parser.add_argument("--warmup_ratio", default=0.2, type=float, help="Linear warmup over warmup_ratio * total_steps.")
parser.add_argument("--valid_steps", default=100, type=int, required=False, help="evaluate frequecny.")
parser.add_argument("--logging_steps", default=1, type=int, help="log interval.")
parser.add_argument("--img_log_dir", default='../logs/reward_model', type=str, help="Logging image path.")
parser.add_argument("--img_log_name", default='reward_model.log', type=str, help="Logging image file name.")
args = parser.parse_args()

logger_writer = LoggerWriter(log_path=args.img_log_dir, log_name=args.img_log_name)
today = datetime.datetime.now().strftime("%Y-%m-%d")
logging.basicConfig(filename=f'./logs/reward_model/reward_model_{today}.log',
                    level=logging.INFO,
                    format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    filemode='a'
                    )
logging.info(device)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = (labels == preds).sum() / len(labels)
    return {
        'accuracy': acc,
    }


def evaluate_model(model, data_loader):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å½“å‰æ¨¡åž‹çš„è®­ç»ƒæ•ˆæžœã€‚

    Args:
        model: å½“å‰æ¨¡åž‹
        data_loader: æµ‹è¯•é›†çš„dataloader
    """
    model.eval()
    with torch.no_grad():
        batch_rank_rewards = []
        for batch in data_loader:
            for batch_idx in range(len(batch['input_ids'])):
                rank_texts_count = len(batch['input_ids'][batch_idx])
                rank_rewards = []
                for text_idx in range(rank_texts_count):
                    reward = model(
                        batch['input_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
                        batch['token_type_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
                        batch['attention_mask'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
                        batch['position_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
                    )
                    rank_rewards.append(reward[0])  # (rank_text_num)
                batch_rank_rewards.append(rank_rewards)  # (batch, rank_text_num)
    model.train()
    total_ranklist, right_ranklist = 0, 0
    for rank_rewards in batch_rank_rewards:
        rank_rewards = [t.cpu().float() for t in rank_rewards]
        rank_rewards_sorted = sorted(rank_rewards, reverse=True)
        total_ranklist += 1
        if rank_rewards_sorted == rank_rewards:
            right_ranklist += 1
    return right_ranklist / total_ranklist


def train():
    encoder = AutoModel.from_pretrained(args.model)
    config = AutoConfig.from_pretrained(args.model)
    model = RewardModel(encoder=encoder, config=config)
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model_size = sum(t.numel() for t in model.parameters())
    logging.info(f"model size: {model_size / 1000 ** 2:.1f}M parameters")
    encoder.to(device)
    model.to(device)
    # dataset = load_dataset('text', data_files={'train': args.train_path,
    #                                            'val': args.dev_path})
    # print(dataset)
    # # dataset = dataset_process(dataset=dataset, tokenizer=tokenizer, max_seq_len=args.max_seq_len)
    # convert_func = partial(dataset_process, tokenizer=tokenizer, max_seq_len=args.max_seq_len)
    # dataset = dataset.map(convert_func)  # batched=Trueé‡‡ç”¨æ‰¹é‡å¤„ç†çš„æ–¹å¼ï¼Œæå‡performance
    #
    # train_dataset = dataset["train"]
    # eval_dataset = dataset["val"]
    # åŠ è½½è®­ç»ƒé›†
    # texts = io.open(args.train_path, encoding='UTF-8').read().strip().split('\n')
    # train_dataset = TextRewardDataset(texts=texts, tokenizer=tokenizer, max_len=args.max_seq_len)
    # # åŠ è½½æµ‹è¯•é›†
    # texts = io.open(args.val_path, encoding='UTF-8').read().strip().split('\n')
    # val_dataset = TextRewardDataset(texts=texts, tokenizer=tokenizer, max_len=args.max_seq_len)
    # train_dataloader = DataLoader(train_dataset, shuffle=False, collate_fn=default_data_collator,
    #                               batch_size=args.batch_size)
    # eval_dataloader = DataLoader(val_dataset, shuffle=False, collate_fn=default_data_collator,
    #                              batch_size=args.batch_size)

    # åŠ è½½Prompt + Ranked Answeræ•°æ®é›†ï¼ŒåŸºäºŽäººå·¥æŽ’åºæ‰€åŠ¨æ€ç”Ÿæˆçš„:prompt-ranked answerã€‚
    # æ–‡ä»¶ç»“æž„ï¼š[['Q1-A(1,1)','Q1-A(1,2)','Q1-A(1,3)','Q1-A(1,4)','Q1-A(1,5)'],
    #          ['Q2-A(2,1)','Q2-A(2,2)','Q2-A(2,3)','Q2-A(2,4)','Q2-A(2,5)'],
    #          ...
    #         ]
    # å¹¶ç”ŸæˆTextRewardæ•°æ®é›†
    content = open(args.train_path, 'r', encoding='utf8').read()
    prompt_ranked_answers = json.loads(content)
    texts = []
    for _, v in prompt_ranked_answers.items():
        text = ""
        prompt = v['prompt']
        for ranked_answer in v['ranked_answers']:
            text += prompt + ranked_answer + "\t"
        texts.append(text)
    train_dataset = TextRewardDataset(texts=texts, tokenizer=tokenizer, max_len=args.max_seq_len)

    content = open(args.val_path, 'r', encoding='utf8').read()
    prompt_ranked_answers = json.loads(content)
    texts = []
    for _, v in prompt_ranked_answers.items():
        text = ""
        prompt = v['prompt']
        for ranked_answer in v['ranked_answers']:
            text += prompt + ranked_answer + "\t"
        texts.append(text)
    val_dataset = TextRewardDataset(texts=texts, tokenizer=tokenizer, max_len=args.max_seq_len)

    train_dataloader = DataLoader(train_dataset, shuffle=False, collate_fn=default_data_collator,
                                  batch_size=args.batch_size)
    eval_dataloader = DataLoader(val_dataset, shuffle=False, collate_fn=default_data_collator,
                                 batch_size=args.batch_size)

    # å¼€å§‹è®­ç»ƒ
    model.train()
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": args.weight_decay,
        },
        {
            "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
    ]
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)

    # æ ¹æ®è®­ç»ƒè½®æ•°è®¡ç®—æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œä»¥ä¾¿äºŽscheduleråŠ¨æ€è°ƒæ•´lr
    num_update_steps_per_epoch = len(train_dataloader)
    max_train_steps = args.epochs * num_update_steps_per_epoch
    warm_steps = int(args.warmup_ratio * max_train_steps)
    lr_scheduler = get_scheduler(
        name='linear',
        optimizer=optimizer,
        num_warmup_steps=warm_steps,
        num_training_steps=max_train_steps,
    )

    training_args = TrainingArguments(
        output_dir=args.save_dir,  # output directory ç»“æžœè¾“å‡ºåœ°å€
        num_train_epochs=args.epochs,  # total # of training epochs è®­ç»ƒæ€»æ‰¹æ¬¡
        per_device_train_batch_size=args.batch_size,  # batch size per device during training è®­ç»ƒæ‰¹å¤§å°
        per_device_eval_batch_size=args.batch_size,  # batch size for evaluation è¯„ä¼°æ‰¹å¤§å°
        evaluation_strategy="steps",  # Evaluation is done at the end of each epoch. or 10 steps
        logging_dir=args.img_log_dir,  # directory for storing logs æ—¥å¿—å­˜å‚¨ä½ç½®
        logging_strategy='epoch',
        learning_rate=args.learning_rate,  # å­¦ä¹ çŽ‡
        save_strategy='epoch',  # ä¸ä¿å­˜æ£€æŸ¥ç‚¹
        save_total_limit=1,  # åªä¿ç•™ä¸€ä¸ªcheckpoint
        overwrite_output_dir=True,  # è¦†ç›–ä¹‹å‰å†™çš„æ¨¡åž‹è¾“å‡ºæ–‡ä»¶
        prediction_loss_only=True,  # åªè®¡ç®—lossä¸è®¡ç®—evaluation
        gradient_accumulation_steps=256 / args.batch_size,
        # æ˜¾å­˜é‡è®¡ç®—æ˜¯å…¸åž‹çš„ç”¨æ—¶é—´æ¢ç©ºé—´ï¼Œæ¯”å¦‚æˆ‘ä»¬å¸Œæœ›è·‘256çš„å¤§ç‚¹çš„batchï¼Œä¸å¸Œæœ›è·‘32è¿™æ ·çš„å°batchï¼Œ
        # å› ä¸ºè§‰å¾—å°batchä¸ç¨³å®šï¼Œä¼šå½±å“æ¨¡åž‹æ•ˆæžœï¼Œä½†æ˜¯gpuæ˜¾å­˜åˆæ— æ³•æ”¾ä¸‹256çš„batchsizeçš„æ•°æ®ï¼Œ
        # æ­¤æ—¶æˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œæ˜¾å­˜é‡è®¡ç®—ï¼Œå°†è¿™ä¸ªå‚æ•°è®¾ç½®ä¸º256/32=8å³å¯ã€‚
        # ç”¨torchå®žçŽ°å°±æ˜¯forwardï¼Œè®¡ç®—loss 8æ¬¡ï¼Œç„¶åŽå†optimizer.step()
    )

    # callè‡ªå®šä¹‰Trainerï¼Œé‡å†™äº†compute_loss
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    trainer = RewardModelTrainer(
        model=model,  # the instantiated ðŸ¤— Transformers model to be trained éœ€è¦è®­ç»ƒçš„æ¨¡åž‹
        tokenizer=tokenizer,
        args=training_args,  # training arguments, defined above è®­ç»ƒå‚æ•°
        train_dataset=train_dataset,  # training dataset è®­ç»ƒé›†
        eval_dataset=val_dataset,  # evaluation dataset æµ‹è¯•é›†
        optimizers=(optimizer, lr_scheduler),  # è‡ªå®šä¹‰ä¼˜åŒ–å™¨
        data_collator=data_collator,  # ä½¿ç”¨åŠ¨æ€paddingï¼ŒèŠ‚çœè®­ç»ƒå†…å­˜å ç”¨
        compute_metrics=compute_metrics  # è®¡ç®—æŒ‡æ ‡æ–¹æ³•
    )

    # print(torch.cuda.memory_summary())
    trainer.train()

    # loss_list = []
    # tic_train = time.time()
    # global_step, best_acc = 0, 0
    # for epoch in range(1, args.epochs + 1):
    #     print(f"epoch: {epoch}")
    #     for _, batch in enumerate(tqdm(train_dataloader)):
    #         batch_rank_rewards = []
    #         for batch_idx in range(len(batch['input_ids'])):
    #             rank_texts_count = len(batch['input_ids'][batch_idx])
    #             rank_rewards = []
    #             for text_idx in range(rank_texts_count):
    #                 reward = model(
    #                     batch['input_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
    #                     batch['token_type_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
    #                     batch['attention_mask'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
    #                     batch['position_ids'][batch_idx][text_idx].unsqueeze(dim=0).to(device),
    #                 )
    #                 rank_rewards.append(reward[0])  # (rank_text_num)
    #             batch_rank_rewards.append(rank_rewards)  # (batch, rank_text_num)
    #         loss = compute_rank_list_loss(batch_rank_rewards)
    #         loss.backward()
    #         optimizer.step()
    #         lr_scheduler.step()
    #         optimizer.zero_grad()
    #         loss_list.append(float(loss.cpu().detach()))
    #
    #         global_step += 1
    #         if global_step % args.logging_steps == 0:
    #             time_diff = time.time() - tic_train
    #             loss_avg = sum(loss_list) / len(loss_list)
    #             logger_writer.add_scalar('train/train_loss', loss_avg, global_step)
    #             print("global step %d, epoch: %d, loss: %.5f, speed: %.2f step/s"
    #                   % (global_step, epoch, loss_avg, args.logging_steps / time_diff))
    #             tic_train = time.time()
    #
    #         if global_step % args.valid_steps == 0:
    #             # å…ˆæ¸…ç©ºæ¨¡åž‹å­˜å‚¨æ–‡ä»¶å¤¹
    #             shutil.rmtree(args.save_dir)
    #             os.mkdir(args.save_dir)
    #             # ä¿å­˜æ–°ä¸€è½®stepçš„æ¨¡åž‹å‚æ•°ã€‚
    #             cur_save_dir = os.path.join(args.save_dir, "epoch_%d" % global_step)
    #             if not os.path.exists(cur_save_dir):
    #                 os.makedirs(cur_save_dir)
    #             torch.save(model, os.path.join(cur_save_dir, 'model.pt'))
    #             # model.save_pretrained(cur_save_dir)
    #             tokenizer.save_pretrained(cur_save_dir)
    #             acc = evaluate_model(model, eval_dataloader)
    #             logger_writer.add_scalar('eval/accuracy', acc, global_step)
    #             logger_writer.record()
    #             print("Evaluation acc: %.5f" % (acc))
    #             if acc > best_acc:
    #                 print(
    #                     f"best F1 performence has been updated: {best_acc:.5f} --> {acc:.5f}"
    #                 )
    #                 best_acc = acc
    #                 cur_save_dir = os.path.join(args.save_dir, "model_best")
    #                 if not os.path.exists(cur_save_dir):
    #                     os.makedirs(cur_save_dir)
    #                 torch.save(model, os.path.join(cur_save_dir, 'model.pt'))
    #                 # model.save_pretrained(cur_save_dir)
    #                 tokenizer.save_pretrained(cur_save_dir)
    #             tic_train = time.time()


if __name__ == '__main__':
    train()
