# coding=UTF-8
# reward modelâ€”â€”å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒ

'''
@File: reward_model_train
@Author: WeiWei
@Time: 2023/2/26
@Email: weiwei_519@outlook.com
@Software: PyCharm
'''

import torch
from torch.utils.data import DataLoader, Dataset
import os
import time
import numpy as np
from transformers import T5Tokenizer, T5ForConditionalGeneration
from transformers import Trainer, TrainingArguments
from utils.gpu_track import MemTracker
import inspect
import logging

# device : GPU or CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
logging.basicConfig(level=logging.INFO)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = (labels == preds).sum() / len(labels)
    return {
        'accuracy': acc,
    }

def model_train(
        tokenizer, model, train_dataset, val_dataset, batch_size, epochs, learning_rate, device,
        model_dir="./models/Company-RewardModel0.1-Chinese/",
        log_dir='./logs/reward_model_train/',
        datasets_dir='./datasets/IMDB_movie/aclImdb/',
):
    # å‚æ•°å‚è€ƒè¿™ç¯‡æ–‡ç« ï¼šhttps://zhuanlan.zhihu.com/p/363670628
    training_args = TrainingArguments(
        output_dir=model_dir,  # output directory ç»“æœè¾“å‡ºåœ°å€
        num_train_epochs=epochs,  # total # of training epochs è®­ç»ƒæ€»æ‰¹æ¬¡
        per_device_train_batch_size=batch_size,  # batch size per device during training è®­ç»ƒæ‰¹å¤§å°
        per_device_eval_batch_size=batch_size,  # batch size for evaluation è¯„ä¼°æ‰¹å¤§å°
        evaluation_strategy="steps",  # Evaluation is done at the end of each epoch. or 10 steps
        logging_dir=log_dir,  # directory for storing logs æ—¥å¿—å­˜å‚¨ä½ç½®
        logging_strategy='epoch',
        learning_rate=learning_rate,  # å­¦ä¹ ç‡
        save_strategy='epoch',  # ä¸ä¿å­˜æ£€æŸ¥ç‚¹
        save_total_limit=1,  # åªä¿ç•™ä¸€ä¸ªcheckpoint
        overwrite_output_dir=True,  # è¦†ç›–ä¹‹å‰å†™çš„æ¨¡å‹è¾“å‡ºæ–‡ä»¶
        gradient_accumulation_steps=256 / batch_size,
        # æ˜¾å­˜é‡è®¡ç®—æ˜¯å…¸å‹çš„ç”¨æ—¶é—´æ¢ç©ºé—´ï¼Œæ¯”å¦‚æˆ‘ä»¬å¸Œæœ›è·‘256çš„å¤§ç‚¹çš„batchï¼Œä¸å¸Œæœ›è·‘32è¿™æ ·çš„å°batchï¼Œ
        # å› ä¸ºè§‰å¾—å°batchä¸ç¨³å®šï¼Œä¼šå½±å“æ¨¡å‹æ•ˆæœï¼Œä½†æ˜¯gpuæ˜¾å­˜åˆæ— æ³•æ”¾ä¸‹256çš„batchsizeçš„æ•°æ®ï¼Œ
        # æ­¤æ—¶æˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œæ˜¾å­˜é‡è®¡ç®—ï¼Œå°†è¿™ä¸ªå‚æ•°è®¾ç½®ä¸º256/32=8å³å¯ã€‚
        # ç”¨torchå®ç°å°±æ˜¯forwardï¼Œè®¡ç®—loss 8æ¬¡ï¼Œç„¶åå†optimizer.step()
    )

    trainer = Trainer(
        model=model,  # the instantiated ğŸ¤— Transformers model to be trained éœ€è¦è®­ç»ƒçš„æ¨¡å‹
        tokenizer=tokenizer,
        args=training_args,  # training arguments, defined above è®­ç»ƒå‚æ•°
        train_dataset=train_dataset,  # training dataset è®­ç»ƒé›†
        eval_dataset=val_dataset,  # evaluation dataset æµ‹è¯•é›†
        compute_metrics=compute_metrics  # è®¡ç®—æŒ‡æ ‡æ–¹æ³•
    )

    trainer.train()
